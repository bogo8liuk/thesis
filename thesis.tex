\documentclass[10pt,a4paper]{article}
\usepackage[italian]{babel}
\usepackage{newlfont}
\usepackage{listings}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{proof}

\lstset{
    language=Haskell,
    basicstyle={\small\ttfamily}
}

\tikzstyle{process} =
    [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=white!30]
\tikzstyle{object} =
    [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=white!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

\textwidth=450pt\oddsidemargin=0pt

\begin{document}
\begin{titlepage}

\begin{center}

{{\Large {\textsc {Alma Mater Studiorum $\cdot$ Universit\`a di Bologna}}}} \rule[0.1cm]{15.8cm}{0.1mm}

\rule[0.5cm]{15.8cm}{0.6mm}
{\small {\bf SCUOLA DI SCIENZE\\
Corso di Laurea in Nome corso di Laurea }}

\end{center}

\vspace{15mm}
\begin{center}

{\LARGE
    {\bf COMPILATORE PER LINGUAGGIO DI}
}\\
\vspace{3mm}
{\LARGE
    {\bf PROGRAMMAZIONE FUNZIONALE}
}\\
\vspace{3mm}
{\LARGE
    {\bf SPERIMENTALE}
}\\

\end{center}

\vspace{40mm}
\par
\noindent
\begin{minipage}[t]{0.47\textwidth}

{\large
    {\bf Relatore:\\
        Chiar.mo Prof.\\
        CLAUDIO SACERDOTI COEN
    }
}

\end{minipage}

\hfill
\begin{minipage}[t] {0.47\textwidth}\raggedleft
{\large
    {\bf Presentata da:\\
        LUCA BORGHI
    }
}

\end{minipage}

\vspace{20mm}
\begin{center}

{\large
    {\bf Sessione\\
        III sessione
        2021-2022
    }
}%2018-2019

\end{center}

\end{titlepage}

\textwidth=450pt\oddsidemargin=0pt

\section{Introduzione}

%TODO: dove viene descritto il problema, la subsection può essere rimossa
\subsection{Il linguaggio}

\subsection{Haskell come modello}
Il compilatore è stato sviluppato mediante il linguaggio \textit{Haskell}, anch'esso linguaggio di programmazione funzionale.
Inoltre, il linguaggio target di langgg è \textit{Core}, un formalismo fortemente basato su \textit{System F}, una
variante tipata del lambda-calcolo che introduce un meccanismo di quantificazione universale sui tipi. Core, inoltre,
viene utilizzato da GHC (compilatore Haskell) come rappresentazione intermedia di Haskell. All'interno di questo
contesto, GHC espone delle API che permettono al cliente di utilizzare le funzionalità del compilatore (cfr. [3]); è quindi
possibile creare e manipolare programmi Core mediante le API, le quali sono state scritte in Haskell. Per quest'ultimo
motivo, Haskell è stato scelto come linguaggio di implementazione del compilatore di langgg, tuttavia, nel contesto
del progetto, Haskell ha anche un altro importante ruolo: alcune delle sue funzionalità sono state, direttamente
o indirettamente, fonte di ispirazione per il design di langgg. Nel prossimo paragrafo vengono presentate le principali
caratteristiche di langgg ed è possibile ritrovare la maggior parte di esse anche in Haskell. Un altro linguaggio
che è stato fonte di ispirazione, ma con minore impatto, è \textit{OCaml}, linguaggio multi-paradigma (funzionale non
puro), soprattutto per la sintassi di langgg e per il costrutto delle polymorphic variants (cfr. paragrafo sugli
sviluppi futuri).

\subsection{Caratteristiche del linguaggio}
Tra le principali caratteristiche del linguaggio vi sono:
\begin{description}
\item[Linguaggio funzionale puro] Come Haskell, langgg è un linguaggio funzionale puro. La nozione di linguaggio
di programmazione \textit{funzionale} è piuttosto lasca e non vi è una vera e propria definizione formale,
tant'è che il termine viene spesso utilizzato (e, talvolta, abusato) per indicare un linguaggio avente alcune
particolari specifiche attribuibili al paradigma di programmazione funzionale. langgg può essere quindi considerato
funzionale poiché, semplicemente, ha numerose caratteristiche proprie del paradigma funzionale, quali: tipi di dati
algebrici, pattern matching, funzioni di ordine superiore, immutabilità, polimorfismo parametrico etc.. Per quanto
riguarda la nozione di \textit{purità}, nel paradigma funzionale viene fatta spesso la distinzione
tra linguaggi puri e impuri; anche qui, non vi sono vere e proprie definizioni e la questione è spesso oggetto di
controversie. Una proposta di definizione è stata fornita da Amr Sabry in [1]: la purità ha a che fare con il passaggio
dei parametri, in particolare, un linguaggio L può essere
considerato puro se, dato un qualsiasi programma p scritto in L, le funzioni di p sono dei "\textit{mapping}" puri
dai valori in input ai valori in output, indipendentemente dal tipo di passaggio dei parametri.
\item[Type-system statico con type-inference] langgg è un linguaggio con type-system statico, inoltre, la fase di
type-checking garantisce la proprietà di type-safety. Il linguaggio permette anche di omettere le indicazioni di
tipo (type-hinting) nella definizione di simboli; in caso non vi sia type-hinting per la definizione di un simbolo,
il compilatore inferirà il tipo "più generale possibile" per il simbolo.
\item["Everything is an expression"] tutti i costrutti all'interno di langgg possono essere considerati espressioni
prive di side-effects; non vi sono costrutti di controllo o costrutti che modificano variabili di stato esterne
al contesto locale. I costrutti principali sono:
    \begin{enumerate}
    \item definizioni di: tipi, variabili, proprietà (che corrispondono alle type-classes di Haskell), istanze di
    proprietà, alias di tipi, firme di simboli; le definizioni di simboli hanno le seguenti grammatiche:
    \begin{lstlisting}
      S :=
          let var args = E

     MS :=
          let var MPM
    \end{lstlisting}
    \item espressioni date dalla seguente grammatica:
    \begin{lstlisting}
      E :=
          var
          datacon
          literal
          E E
          lam args -> E
          lam MPM
          S in E
          MS in E
          match E with PM

     PM :=
          ME -> E | ... | ME -> E

    MPM :=
          | ME ... ME = E | ... | ME ... ME = E

     ME :=
          var
          _
          literal
          datacon ME ... ME
          
    \end{lstlisting}
    \item un costrutto, valutato a compile-time (cfr. parsing degli operatori), per definire una "categoria" di
    operatori. Le categorie di operatori hanno degli identificatori per poterle nominare, inoltre definiscono le
    seguenti proprietà:
        \begin{enumerate}
        \item una lista di operatori appartenenti alla proprietà. Sono compresi gli identificatori di variabili (gli
        operatori stessi sono identificatori di variabili) che possono essere utilizzati con la sintassi degli operatori;
        \item una lista di categorie di operatori i quali avranno meno precedenza degli operatori della categoria
        corrente;
        \item una lista di categorie di operatori i quali avranno più precedenza degli operatori della categoria
        corrente;
        \item la fissità degli operatori.
        \end{enumerate}
    \end{enumerate}
\item[Tipi di dati algebrici] langgg supporta anche i tipi di dati algebrici (che sono l'unico modo per definire nuovi
tipi). Con essi, vi è anche la nozione di \textit{data constructor}, ovvero un costrutto dotato di tipo che può essere
accompagnato da dati. Per eseguire operazioni con i tipi di dati algebrici vi è il costrutto del pattern matching
(cfr. grammatica delle espressioni) che permette di destrutturare un data constructor dai suoi dati associati.
\item[Polimorfismo parametrico] il sistema di tipi del linguaggio supporta le variabili di tipo. Questo tipo di
polimorfismo permette di avere singoli algoritmi per una moltitudine di tipi.
\item[Kind] Il linguaggio permette la manipolazione di \textit{funzioni di tipi}, introducendo la nozione di
\textit{kind} (cfr. sezione sul sistema di tipi). Quindi, data la seguente definizione di tipo:
\begin{lstlisting}
type M a b = DataCon1 a | DataCon2 b | DataCon3 a Char
\end{lstlisting}
dove \textit{a} e \textit{b} sono variabili di tipo, è possibile, ad esempio, avere tipi della forma:
\begin{lstlisting}
M Int Char
M a b
M String
M x
M
\end{lstlisting}
\`E possibile altresì avere un tipo della forma:
\begin{lstlisting}
m Int
\end{lstlisting}
dove \textit{m} è una variabile di tipo a cui viene applicata il tipo \textit{Int}.
\item[Polimorfismo ad hoc] attraverso il meccanismo delle type-classes (all'interno del linguaggio vengono chiamate
proprietà), è possibile creare funzioni polimorfe che possono essere applicate ad argomenti di tipi differenti e, a
seconda dei tipi degli argomenti, viene "selezionata" la giusta implementazione. Ogni proprietà può avere uno, ma anche
più tipi associati; inoltre, i tipi associati alle proprietà possono essere anche variabili di tipo.

\end{description}

\section{Il compilatore}

\subsection{La struttura}
\begin{tikzpicture}[node distance=2cm]
\node (src) [object] {Codice sorgente};
\node (parser) [process, below of=src] {Parser};
\draw [arrow] (src) -- (parser);
\node (checker) [process, below of=parser] {Checker della correttezza};
\draw [arrow] (parser) -- (checker);
\node (typedBuilder) [process, below of=checker] {Costruzione dei token tipati};
\draw [arrow] (checker) -- (typedBuilder);
\node (prepare) [process, below of=typedBuilder] {Preparazione alla type inference};
\draw [arrow] (typedBuilder) -- (prepare);
\node (typeInf) [process, below of=prepare] {Type inference - type checking};
\draw [arrow] (prepare) -- (typeInf);
\node (CoreGen) [process, below of=typeInf] {Generazione codice Core};
\draw [arrow] (typeInf) -- (CoreGen);
\node (backend) [process, below of=CoreGen] {Back-end};
\draw [arrow] (CoreGen) -- (backend);
\node (exe) [process, below of=backend] {Eseguibile};
\draw [arrow] (backend) -- (exe);
\node (desugar) [process, right of=prepare, xshift=7cm] {Desugaring};
\draw [arrow] (desugar) -- (checker);
\draw [arrow] (desugar) -- (typedBuilder);
\draw [arrow] (desugar) -- (prepare);
\draw [arrow] (desugar) -- (typeInf);
\draw [arrow] (desugar) -- (CoreGen);
\end{tikzpicture}
\newline

La struttura del compilatore è sequenziale, con eccezione fatta per la fase di \textit{desugaring}. Quest'ultima si
occupa di rimuovere il cosiddetto "zucchero sintattico" dalle strutture dati - che mantengono le informazioni del
programma - che, durante le varie fasi di compilazione, possono subire semplificazioni. Talvolta, alcune semplificazioni
vengono posticipate il più possibile per permettere al compilatore di restituire in output messaggi d'errore comprensibili
per l'utente. Un esempio ne è la trasformazione che subiscono le strutture dati che rappresentano il costrutto del
pattern matching. Il modulo che si occupa del desugaring si presenta come una libreria che espone delle API che agiscono
sulle strutture dati del compilatore. \`E compito del compilatore fare le chiamate al modulo di desugaring. Di seguito,
vi è una più ampia e precisa descrizione delle singole fasi di compilazione, con enfasi particolare sugli algoritmi
più interessanti utilizzati per risolvere i singoli sottoproblemi e su come le varie fasi interagiscono tra loro.

\section{L'albero di sintassi astratta}
Nella prima parte del compilatore viene eseguito il parsing del sorgente.
Questa operazione produce quindi l'albero di sintassi astratta (AST). Il codice che gestisce i "token" dell'AST è
all'interno del modulo \texttt{Compiler.Ast.Tree}; l'entry point dell'albero è dato dal token \texttt{Program}, il
quale, come nodi figli, ha delle \texttt{Declaration} che rappresentano i vari costrutti del linguaggio.

\begin{lstlisting}
newtype Program a = Program [Declaration a]

data Declaration a =
      ADT (AlgebraicDataType a)
    | AliasADT (AliasAlgebraicDataType a)
    | Intf (Interface a)
    | Ins (Instance a)
    | Sig (Signature a)
    | Let (SymbolDeclaration a)
    | LetMulti (MultiSymbolDeclaration a)
\end{lstlisting}

Di seguito vi è lo schema della prima parte del compilatore: \newline

\begin{tikzpicture}[node distance=2cm]
\node (src) [object] {Codice sorgente};
\node (rawast) [object, below of=src] {Albero di sintassi astratta};
\draw [arrow] (src) -- node[anchor=west] {Parser} (rawast);
\node (rawastBuiltin) [object, below of=rawast] {Albero di sintassi astratta};
\draw [arrow] (rawast) -- node[anchor=west] {Aggiunta token built-in} (rawastBuiltin);
\node (rawastNames) [object, below of=rawastBuiltin] {Albero di sintassi astratta};
\draw [arrow] (rawastBuiltin) -- node[anchor=west] {Check esistenza dei nomi} (rawastNames);
\node (rawastArgs) [object, below of=rawastNames] {Albero di sintassi astratta};
\draw [arrow] (rawastNames) -- node[anchor=west] {Check numero degli argomenti} (rawastArgs);
\node (rawastAlias) [object, below of=rawastArgs] {Albero di sintassi astratta};
\draw [arrow] (rawastArgs) -- node[anchor=west] {Sostituzione degli alias di tipo} (rawastAlias);
\node (desugar) [object, right of=rawastAlias, xshift=7cm] {Desugaring};
\draw [arrow] (desugar) -- (rawastAlias);
\end{tikzpicture}

\subsection{Il parser}
Il parser è la prima componente del compilatore (dopo la lettura del sorgente). \`E stato scritto mediante la libreria
open-source \texttt{Parsec} [4], la quale si basa sul concetto di parser combinator monadico. Il codice risiede nel
modulo \texttt{Compiler.Syntax} ed ha una struttura gerarchica: parte definendo i combinators di "pezzi" primitivi dei
token dell'AST (\texttt{Compiler.Syntax.Lib.SimpleParser}), dopodiché, nel modulo \texttt{Compiler.Syntax.Grammar} vi
è la generazione vera e propria dei token dell'AST e, infine, vi è l'entry point del parser, ovvero
\texttt{Compiler.Syntax.Parser}. \`E bene notare che all'interno del modulo \texttt{Compiler.Syntax.Lib.SimpleParser}
non sono visibili le API dell'AST, in quanto esso si occupa soltanto del parsing dei costrutti del linguaggio e non
della generazione dei token.

\subsubsection{Parsing degli operatori}
langgg permette di definire operatori che vengono trattati come simboli di variabili (funzioni). In questo contesto,
il linguaggio espone all'utente un costrutto particolare che permettere di definire le proprietà degli eventuali
operatori. Questo costrutto viene valutato durante il parsing e definisce una categoria di operatori, ecco un esempio:

\begin{lstlisting}
OPERATORS_CATEGORY {#
    name : Application ;
    operators : |>, <|, `applyTo ;
    lesser than : Comparison, Numeric ;
    greater than : Functor ;
    fixity : InfixLeft ;
#}
\end{lstlisting}

Il primo "campo" è il nome della categoria ed è utile per poter identificare la categoria, infatti, il terzo e il
quarto campo definiscono rispetto a quali categorie gli attuali operatori hanno precedenza. Il secondo campo definisce
l'insieme di operatori che fanno parte della categoria. Infine, l'ultimo campo è una costante e definisce la fissità
degli operatori: infissa senza associazione, infissa con associazione a sinistra, infissa con associazione a destra,
postfissa o prefissa. La versione infissa è solo per gli operatori binari, mentre postfissa e prefissa solo per gli
operatori unari. Si può notare che nella lista di operatori vi è anche un simbolo di variabile preceduto dal simbolo
di backtick; questo è possibile, in quanto langgg permette di utilizzare i simboli di variabili come operatori
facendoli precedere dal carattere backtick. La valutazione di questo costrutto è interamente integrata nel parser ed
è obbligatorio per l'utente definire le categorie degli operatori all'inizio del sorgente. La libreria \texttt{Parsec}
offre delle API anche per il parsing degli operatori; l'algoritmo di gestione delle categorie si occupa di costruire
una tabella di operatori, implementata semplicemente come una lista di liste, ordinando i gruppi (di categorie) in
base alla loro precedenza. Le informazioni sulle categorie vengono estrapolate dal parsing del costrutto e vengono
passate successivamente all'algoritmo di ordinamento delle categorie. Quest'ultimo si può ridurre all'inserimento
di un elemento in una lista di liste:

\begin{lstlisting}
insert(x, ll):
    match ll with
        [] -> [[x]]
        (l :: lt) ->
            if any x' in l. x' < x        // Condizione d'inserimento (1)
            then
                if areAmbigous(x, ll)
                then fail
                else [x] :: l :: lt
            else if all x' in l. x' == x  // Condizione d'inserimento (2)
            then
                if areAmbigous(x,ll)
                then fail
                else subInsert(x, l) :: lt
            else l :: insert(x, lt)
\end{lstlisting}

L'algoritmo scorre la lista finché non:
\begin{enumerate}
    \item trova una sottolista \textit{l} in cui esiste almeno un elemento minore di \textit{x}; in questo caso, controlla
    eventuali ambiguità delle categorie, poiché l'utente potrebbe aver definito categorie tali che la nozione di
    ordinamento tra loro non è transitiva. La funzione \texttt{areAmbigous} nel pezzo di pseudocodice si occupa
    di eseguire questo controllo. Se non esistono ambiguità tra le categorie, allora viene creata una nuova lista
    singoletto contenente \textit{x} che viene inserita davanti alla sottolista \textit{l}
    \item trova una sottolista \textit{l} in cui tutti gli elementi sono uguali a \textit{x}; in questo caso, \textit{x}
    viene inserito in \textit{l}, eseguendo sempre prima il controllo sulle ambiguità.
\end{enumerate}
In tutti gli altri casi, la corrente sottolista \textit{l} contiene almeno un elemento $ x' $ tale che
$ x' > x $, quindi l'inserimento non può ancora avvenire. Quando la tabella è completa, essa viene passata alla
funzione \texttt{buildExpressionParser} della libreria \texttt{Parsec} che si occupa di costruire il parser per le
espressioni.

\subsection{Check dei nomi}
Dopo la fase di parsing e dell'aggiunta di token built-in del linguaggio (ad esempio, alcuni operatori aritmetici),
viene effettuato il controllo di esistenza di ogni tipo di simbolo: nomi di tipo, nomi di variabili, nomi di proprietà,
etc.. Perciò deve valere la seguente condizione:
    \[ \forall name \in AST. \exists def(name) \]
dove \textit{def} è la funzione che ritorna la definizione di un token dell'AST.
Questo tipo di check è molto importante in quanto fasi successive del compilatore basano le loro computazioni
sull'ipotesi che tutti i simboli sono stati definiti. Il codice risiede nel modulo \texttt{Compiler.Args}.

\subsection{Check degli argomenti}
Il controllo degli argomenti viene effettuato, a differenza del check dei nomi che viene eseguito su ogni tipo di nome,
solo sui nomi di tipo, di alias e di proprietà. In particolare, devono valere le seguenti condizioni:
\[ \forall name \in AST. \]
\begin{enumerate}
    \item:
        \[ isTypeName(name) \Longrightarrow \# args(name) \leq \#args(def(name)) \]
    \item:
        \[ isAliasName(name) \Longrightarrow \#args(name) = \#args(def(name)) \]
    \item:
        \[ isPropertyName(name) \Longrightarrow \#args(name) = \#args(def(name)) \]
\end{enumerate}
dove \textit{isTypeName}, \textit{isAliasName}, \textit{isPropertyName} sono le funzioni che ritornano \textit{true}
se il nome in input è, rispettivamente, un nome di tipo, un nome di alias, un nome di proprietà, \textit{false} altrimenti,
\textit{args} è la funzione che calcola gli argomenti di un token e il simbolo \textit{\#} è la funzione che calcola
la cardinalità di un insieme. La condizione (1) è meno stringente di (2) e di (3),
in quanto l'utente può "manipolare" non solo tipi, ma anche funzioni di tipi,
anche conosciute come \textit{type constructor}. La condizione (2) è fondamentale per la prossima fase del compilatore.
Il codice risiede nel modulo \texttt{Compiler.Args}.

\subsection{Eliminazione degli alias di tipo}
langgg offre un costrutto che permette all'utente di definire alias di tipi. Ecco un esempio:

\begin{lstlisting}
alias CharAnd x = Tuple2 Char x
\end{lstlisting}

Questo tipo di costrutto viene completamente valutato in fase di compilazione: ogni occorrenza di nome di alias viene
trattata come una vera e propria macro, quindi viene sostituita con il tipo associato all'alias. Il modulo di Desugaring
si occupa di questo task (\texttt{Compiler.Desugar.Alias}).
Come si nota nell'esempio, gli alias ammettono argomenti (variabili di tipo) che vengono passati alla funzione di tipo.
Il linguaggio ammette anche alias di alias, tuttavia, ciò può portare a \textit{cicli} di alias, come ad esempio:

\begin{lstlisting}
alias A = B
alias B = C
alias C = A
\end{lstlisting}

Per questo motivo, l'algoritmo di sostituzione degli alias implementa anche la "cycle detection". Inoltre, nella
sostituzione è fondamentale che valga la condizione sugli alias nel check degli argomenti (cfr. check degli argomenti):
    \[ \forall name \in AST. isAliasName(name) \Longrightarrow \#args(name) = \#args(def(name)) \]
Se la sopracitata condizione non fosse vera, non sarebbe possibile effettuare l'unificazione nella kind-inference (cfr.
kind-inference).

\subsection{Eliminazione delle firme di funzioni}
langgg espone un costrutto, detto \textit{signature} (in italiano "firma"), che permette di indicare il tipo di un
binding. Ad esempio:

\begin{lstlisting}
val id : a -> a
\end{lstlisting}

L'esempio appena mostrato indica che la variabile \textit{id} ha tipo $ \forall \alpha. \alpha \mapsto \alpha $. Questo
tipo di costrutto non è nient'altro che "zucchero sintattico" per il type-hinting dei binding. I token dei binding
- \texttt{SymbolDeclaration} e \texttt{MultiSymbolDeclaration} - possono possedere delle informazioni sul type-hinting,
ad esempio, guardando la definizione di \texttt{MultiSymbolDeclaration}:

\begin{lstlisting}
data MultiSymbolDeclaration a =
    MultiSymTok (SymbolName a) (Hint a) (MultiPatternMatch a) a
\end{lstlisting}

si può notare che il costruttore \texttt{MultiSymTok} prenda in input un token \texttt{Hint}. Il task del modulo
\texttt{Desugar.Sigs} è di eliminare dall'AST i costrutti \texttt{Signature} che rappresentano, appunto, le firme delle
variabili e aggiungere la loro informazione sul tipo come type-hinting delle definizioni delle variabili.

\subsection{Renaming delle variabili di tipo}
Prima di iniziare le fasi di costruzione dei token "tipati", il programma viene visitato e le occorrenze delle variabili
di tipo vengono tutte rinominate in modo che, dato il programma $ P $:
\[ \forall v, v' \in tyVars(P). \; v \neq v' \]
dove $ tyVars $ è la funzione che ritorna tutte le variabili \textit{non legate} di un token, mentre il test di uguaglianza
tra variabili di tipo è definita come il test di uguaglianza degli identificatori delle variabili.
Chiaramente, le variabili legate vengono cambiate in base a come vengono aggiornati i binders, ad esempio, si consideri
la seguente definizione:
\begin{lstlisting}
type T a b = C1 Int a | C2 a (T a b)
\end{lstlisting}
Se viene effettuata la seguente sostituzione:
\[ \{ a \mapsto a_1, b \mapsto a_2 \} \]
Allora la nuova definizione di T avrà la seguente forma:
\begin{lstlisting}
type T a1 a2 = C1 Int a1 | C2 a1 (T a1 a2)
\end{lstlisting}
Il motivo di questo renaming verrà descritto in seguito (cfr. Kind-inference). Il codice risiede nel modulo
\texttt{Compiler.Desugar.TyVars}.

\section{Generazione dei token "tipati"}
Dopo la generazione dell'albero di sintassi astratta e alcune fasi di desugaring, questa seconda macro componente del
compilatore si occupa della generazione dei token "tipati". Questi ultimi prendono questa nomea in quanto, a questo
livello, compaiono le nozioni di tipo e di kind del linguaggio.

\begin{tikzpicture}[node distance=2cm]
\node (desast) [object] {AST "dezuccherato"};
\node (desastContsCheck) [object, below of=desast] {AST "dezuccherato"};
\draw [arrow] (desast) -- node[anchor=east] {Check dei constraints} (desastContsCheck);
\node (typesTable) [object, below of=desastContsCheck] {Tabella dei tipi};
\draw [arrow] (desastContsCheck) -- node[anchor=east] {Kind-inference} (typesTable);
\node (consTable) [object, below of=typesTable] {Tabella dei costrutturi};
\draw [arrow] (typesTable) -- node[anchor=east] {Costruzione dei data constructor} (consTable);
\node (contsTable) [object, below of=consTable] {Tabella dei constraints o predicati};
\draw [arrow] (consTable) -- node[anchor=east] {Costruzione dei constraints} (contsTable);
\node (instances) [object, below of=contsTable] {Tabelle di: metodi di proprietà; metodi di istanza; istanze};
\draw [arrow] (contsTable) -- node[anchor=east] {Valutazione delle istanze di proprietà} (instances);
\node (bindings) [object, below of=instances] {Bindings dell'AST pronti per la type-inference};
\draw [arrow] (instances) -- node[anchor=east] {"Preparazione" alla type-inference} (bindings);
\node (tyBindings) [object, below of=bindings] {Bindings tipati};
\draw [arrow] (bindings) -- node[anchor=east] {type-inference} (tyBindings);
\node (desugar) [object, right of=bindings, xshift=6cm] {Desugaring};
\draw [arrow] (desugar) -- (instances);
\draw [arrow] (desugar) -- (bindings);
\draw [arrow] (desugar) -- (tyBindings);
\end{tikzpicture}

\subsection{Approccio a tabelle}
A differenza della prima macro componente del compilatore, dove l'unica struttura dati di primo livello era l'AST, in
questo caso vi sono molteplici strutture dati di primo livello. Innanzitutto, nel modulo \texttt{Compiler.Ast.Typed},
vi sono le definizioni di tutti i token tipati e le operazioni su di essi; proprio in questo modulo compaiono:
\begin{enumerate}
    \item le nozioni che riguardano i tipi del linguaggio:
    \begin{lstlisting}
data LangKind               --kind
data LangVarType a          --variabile di tipo
data LangHigherType a       --mono-tipo
data LangSpecConstraint a   --"constraint" o predicato
data LangQualType a         --mono-tipo qualificato
data LangTypeScheme a       --poli-tipo o schema di tipo
    \end{lstlisting}
    \item i token tipati che costituiscono un programma:
    \begin{lstlisting}
data NotedVar a           --variabili
data NotedVal a           --valori: letterali e data constructor
data NotedMatchExpr a     --espressioni per il pattern match
data NotedExpr a          --expressioni
    \end{lstlisting}
    \item operazioni che riguardano la manipolazione dei tipi quali unificazione, test di specificità, specializzazione,
    instanziazione, generalizzazione. Inoltre, vi sono le funzioni e le strutture dati per gestire il dispatch statico.
\end{enumerate}
Nonostante la presenza di token tipati, non esiste una corrispondente versione tipata dell'AST con un unico entry point,
bensì le informazioni che riguardano un programma vengono memorizzate in "tabelle" (cfr. modulo
\texttt{Compiler.Types.Tables}):
\begin{lstlisting}
newtype TypesTable a          --tabella dei "modelli" di tipi
newtype DataConsTable a       --tabella dei data constructor
newtype ConstraintsTable a    --tabella dei "modelli" di constraints
newtype InstsTable a          --tabella dei bindings delle istanze
newtype PropMethodsTable a    --tabella dei metodi di proprieta'
newtype ImplTable a           --tabella delle istanze
data    TypedProgram a        --tabella dei bindings tipati
\end{lstlisting}
Vedremo nel dettaglio ogni tabella nelle descrizioni delle varie fasi del compilatore. Tuttavia, è necessaria una nota
su \texttt{TypesTable} e \texttt{ConstraintsTable}. Come si legge dai commenti, esse sono tabelle per memorizzare dei
"modelli". Tali modelli sono necessari alla costruzione dei tipi e dei predicati all'interno di un programma. Ad esempio,
date le seguenti definizioni in langgg:
\begin{lstlisting}
type Box a = Boxing a
property Stateful m =
    val getState : m a -> a
;;
\end{lstlisting}
Verranno creati dei token (presenti in \texttt{Compiler.Ast.Typed}) dei tipi:
\begin{lstlisting}
data LangNewType a            --type constructor
data LangNewConstraint a      --constraint constructor
\end{lstlisting}
che rappresenteranno rispettivamente il modello per tipi \texttt{Box} e il modello per constraints \texttt{Stateful} e che
verranno memorizzati nelle suddette tabelle.

Sempre nel modulo \texttt{Compiler.Types.Tables}, viene definito anche il cosiddetto "binding tipato":
\begin{lstlisting}
type BindingSingleton a = (NotedVar a, [NotedVar a], NotedExpr a)
data TypedBinding a =
      TyNonRec (BindingSingleton a)
    | TyRec [BindingSingleton a]
\end{lstlisting}
Osservando l'implementazione di \texttt{TypedBinding}, si nota come esistano due tipi di binding. Il primo è per i binding
non ricorsivi, mentre il secondo è per i binding che sono mutualmente ricorsivi fra loro (cfr. type-inference).

\subsubsection{Confronto con GHC}
Come è stato detto precedentemente, l'approccio del compilatore è quello di costruire tabelle man mano che le informazioni
vengono inferite dall'AST. GHC (the Glasgow Haskell Compiler) utilizza un approccio differente, in quanto non utilizza
alcuna "symbol table", bensì ogni token tipato (di GHC) nella compilazione di un programma Haskell da parte di GHC può
textit{puntare} ad altri token tipati [5]. Si crea così un grafo di strutture dati tipate. Ad esempio, GHC, per gestire
le entità di type constructor e data constructor, utilizza rispettivamente i token \texttt{TyCon} e \texttt{DataCon} (si
ricordi che GHC è scritto in Haskell):
\begin{lstlisting}
data TyCon
data DataCon
\end{lstlisting}
Ogni token di tipo \texttt{TyCon} punterà a una lista di \texttt{DataCon} che, a loro volta, conterranno la referenza
al loro costruttore di tipo. Come puntualizza Edward Y. Zang nell'introduzione dell'articolo [6],
uno svantaggio di questo approccio è che il grafo è immutabile e quindi, per poter
aggiornare i nodi del grafo è necessario ricostruire il grafo da zero. Tuttavia, questo problema è mitigato, in quanto
gli aggiornamenti del grafo sono parecchio rari, inoltre, man mano che GHC ottiene informazioni dal programma Haskell,
accrescerà il grafo senza aggiornare i nodi preesistenti; in questo modo, non vi è alcuna necessità di costruire il
grafo da zero.

\subsection{Il sistema di tipi}
In questo paragrafo, verrà presentato il sistema di tipi di langgg. Come è stato già menzionato in precedenza, il
linguaggio supporta il polimorfismo ad hoc e il polimorfismo parametrico; quest'ultimo viene implementato attraverso
il concetto di variabile di tipo, la quale può essere considerata come un "placeholder" per i tipi.

\subsubsection{Mono-tipo}
Di seguito, viene definito il concetto di \textit{mono-tipo}:
\[ MT \; := \; \alpha \; | \; T \; | \; MT \; MT \; | \; MT \mapsto MT \]
dove $ \alpha $ è una variabile di tipo e $ T $ è un costruttore di tipo. Come si può dedurre dalla definizione stessa,
non si deve confondere
il concetto di mono-tipo con quello di tipo \textit{monomorfo}, infatti i mono-tipi, a differenza dei tipi monomorfi,
ammettono le variabili di tipo. L'ultimo caso è il tipo \textit{funzione}; nella sezione sull'inferenza di tipo verrà
mostrato il motivo per il quale è fondamentale garantire la presenza del tipo funzione all'interno del type-system.
Nel compilatore, la definizione di mono-tipo si trova nel modulo \texttt{Compiler.Ast.Typed}:
\begin{lstlisting}
data LangHigherType a =
      LTy (LangType a)
    | HApp [LangHigherType a] a
\end{lstlisting}
dove \texttt{LangType} rappresenta tutti i tipi che possono essere definiti in langgg. Si noti come vi è un secondo
caso (\texttt{HApp}), il quale rappresenta il tipo funzione. Il costruttore \texttt{HApp} prende in input una lista
di mono-tipi, tuttavia il tipo funzione può avere al massimo due argomenti, perciò il modulo \texttt{Compiler.Ast.Typed}
si occupa internamente di rifiutare qualsiasi valore con costruttore \texttt{HApp} che abbia più di due argomenti.
La causa per la quale non vi è un numero fissato di argomenti è la presenza delle funzioni di tipo, infatti,
in questo modo, è possibile esprimere senza difficoltà, all'interno del compilatore, tipi come:
\begin{itemize}
\item \texttt{(->) a}
\item \texttt{(->)}
\end{itemize}

\subsubsection{Tipi higher-kinded}
langgg permette all'utente di utilizzare \textit{funzioni di tipo} (o \textit{costruttori di tipo}). A questo scopo,
viene introdotta la nozione di \textit{kind}, il quale rappresenta un'informazione aggiuntiva per i tipi. Informalmente,
dato un type-system \textit{TS}, un kind-system \textit{KS} si può vedere come un type-system di livello "superiore"
a \textit{TS}. Ora diamo una definizione più precisa di kind:
\[ K \; := \; \kappa \; | \; * \; | \; K \mapsto K \]
dove $ \kappa $ è una variabile di kind e $ * $ è la costante primitiva di kind (detta anche "tipo"), il quale
rappresenta tutti quei tipi che non hanno bisogno di parametri di tipi. Ecco alcuni esempi:
\begin{itemize}
    \item $ * \mapsto * $ è il kind delle funzioni di tipo unarie;
    \item $ (* \mapsto *) \mapsto * \mapsto * $ è il kind delle funzioni di tipo binarie che prendono in input una
    funzione di tipo unaria e un tipo e ritornano un altro tipo.
    \item $ \kappa_1 \mapsto \kappa_2 $ è il kind delle funzioni unarie che prendono in input una funzione di tipo
    di kind $ \kappa_1 $ e ritorna un'altra funzione di tipo di kind $ \kappa_2 $.
\end{itemize}
Data la presenza di tipi \textit{higher-kinded} all'interno di langgg, è doveroso fare un'ulteriore precisazione che
riguarda i mono-tipi: alla definizione di mono-tipo deve essere aggiunta la condizione sulla correttezza dei kind.
Ad esempio, dato il tipo:
\[ t_1 \mapsto t_2 \]
sapendo che il kind del costruttore di tipo funzione è $ * \mapsto * \mapsto * $, è necessario, affinché il suddetto
kind venga rispettato, che il kind di $ t_1 $ e $ t_2 $ sia $ * $.
Nel codice sorgente, la definizione di kind nel compilatore langgg si trova in \texttt{Compiler.Ast.Typed}:
\begin{lstlisting}
data LangKind =
      LKVar String
    | LKConst
    | SubLK [LangKind]
\end{lstlisting}
Il costruttore \texttt{LKVar} rappresenta le variabili di kind, il costruttore \texttt{LKConst} rappresenta il kind
\textit{tipo} $ * $, mentre il costruttore \texttt{SubLK} rappresenta l'applicazione di kinds.

\subsubsection{Subtyping: predicati e tipi qualificati}
La nozione di mono-tipo ammette le variabili di tipo, le quali fungono da placeholder per qualsiasi tipo dello stesso
kind della variabile. Tuttavia, solamente con la nozione di mono-tipo non è possibile fare asserzioni sulle variabili
di tipi, se non, appunto, che sono placeholder adatti a qualsiasi tipo. langgg supporta anche una nozione di sottotipo.
Prendiamo, ad esempio, la seguente variabile di tipo:
\[ \alpha : * \]
la notazione $ : * $ serve, in questo caso, per rendere esplicito il kind della variabile. $ \alpha $ è un placeholder
adatto a tipi quali, ad esempio, \texttt{Int}, \texttt{List b} oppure \texttt{List Char} poiché hanno tutti kind $ * $.
Potrebbe essere "comodo" fare asserzioni su $ \alpha $, ad esempio, si può asserire che $ \alpha $ è un placeholder
valido per tutti quei tipi $ S $ che sono sottotipi di un certo tipo $ T $. Una definizione lasca di sottotipo è la
seguente: se $ S $ è sottotipo di $ T $ (scriviamo $ S \leq T $), allora ogni termine di $ S $ può essere utilizzato
in maniera \textit{safe} in ogni \textit{contesto} in cui un termine di $ T $ è richiesto, dove le definizioni di
\textit{safe} e \textit{contesto} sono dipendenti dal formalismo.
Aggiungiamo una notazione per descrivere questa nozione:
\[ \alpha \leq T . \; \alpha \]
Possiamo quindi "restringere" i possibili tipi adatti a "riempire" il placeholder rappresentato da $ \alpha $. Più in
generale, possiamo considerare $ \alpha \leq T $ come un predicato $ P $ su $ \alpha $. Utilizzeremo, quindi, la seguente
notazione:
\[ P(\alpha) \Rightarrow \alpha \]
Ora abbiamo una sintassi per descrivere \textit{predicati} o \textit{constraint} sulle variabili di tipo. Tale
comportamento si può estendere a qualsiasi forma di tipo, non solo alle variabili di tipo, ma per farlo è necessario
prima definire la sintassi dei predicati. In langgg, un predicato o constraint ha la seguente forma:
\[ C := P \; MT_1 \; ... \; MT_n \]
dove $ P $ è il nome di una proprietà (cfr. polimorfismo ad hoc). Ora possiamo quindi definire un'estensione dei
mono-tipi, in modo che su questi ultimi si possano aggiungere delle ipotesi. Per una definizione più generale possibile,
ammettiamo che su un mono-tipo si possa applicare un numero arbitrario di predicati:
\[ QT := MT \; | \; C \Rightarrow QT \]
Questa appena data è la nozione di \textit{tipo qualificato}. La definizione si trova nel modulo
\texttt{Compiler.Ast.Typed}:
\begin{lstlisting}
data LangQualType a = Qual [LangSpecConstraint a] (LangHigherType a)
\end{lstlisting}

\subsubsection{Schemi di tipi}
Finora abbiamo trattato mono-tipi e tipi qualificati, tuttavia, la sola presenza delle variabili di tipo non è sufficiente
a costruire tipi "polimorfi". Si consideri il seguente esempio:
\[ map : (\alpha \mapsto \beta) \mapsto List \; \alpha \mapsto List \; \beta \]
In questo caso, il tipo della funzione $ map $ non è polimorfo, in quanto le variabili di tipo $ \alpha $ e $ \beta $
rappresentano un tipo fissato non ancora conosciuto. Per ottenere il polimorfismo, è necessario introdurre
dei quantificatori, ad esempio, possiamo rifinire il tipo di $ map $ nel modo seguente:
\[ map : \forall \alpha, \beta . \; (\alpha \mapsto \beta) \mapsto List \; \alpha \mapsto List \; \beta \]
A differenza di prima, in cui $ map $ aveva un tipo prefissato, qui può avere molteplici tipi. Ora diamo la definizione
di \textit{schema di tipo} o \textit{poli-tipo} in langgg:
\[ PT := QT \; | \; \forall \alpha. \; PT \]
Questa definizione lascia spazio alla presenza di \textit{variabili libere}, in quanto non necessariamente tutte le
variabili di tipo che compaiono in un tipo qualificato sono legate a un quantificatore. Le variabili libere vengono
trattate come costanti. Si noti come i quantificatori possono apparire solamente alla testa di un tipo, ad esempio, il
seguente tipo non è accettato in langgg:
\[ \forall \alpha . \; (\forall \alpha . \; \alpha \mapsto \alpha) \mapsto \alpha \]
%TODO: System F
La definizione di schema di tipo in langgg si trova nel modulo \texttt{Compiler.Ast.Typed}:
\begin{lstlisting}
data LangTypeScheme a = Forall [LangVarType a] (LangQualType a)
\end{lstlisting}
La politica sulla sintassi degli schemi di tipi di langgg è omettere il quantificatore $ \forall $. Per questo, ogni qual
volta vi è un costrutto di type-hinting (firme comprese), il tipo viene considerato come uno schema di tipo in cui tutte
le variabili di tipo che compaiono vengono legate a un quantificatore, ad esempio:
\begin{lstlisting}
val map : (a -> b) -> List a -> List b
\end{lstlisting}
in questo caso la funzione \texttt{map} avrà il seguente tipo:
\[ map : \forall \alpha, \beta . \; (\alpha \mapsto \beta) \mapsto List \; \alpha \mapsto List \; \beta \]

\subsubsection{Test di uguaglianza tra tipi}
Il test di uguaglianza tra tipi si differenzia parecchio tra le diverse nozioni di "tipo", inoltre, non sempre risulterà
utile avere una definizione di uguaglianza tra alcune nozioni di tipo. Partendo dalla nozione di mono-tipo, due
mono-tipi $ \tau $ e $ \tau' $ sono uguali se hanno i termini identici. Dato che langgg supporta la nozione di mono-tipo
higher-kinded, è banale asserire che:
\[ kindOf(\tau) \neq kindOf(\tau') \Longrightarrow \tau \neq \tau' \]
Per quanto riguarda i tipi qualificati, è necessario prendere in considerazione anche i predicati che precedono un
mono-tipo. Definiamo, quindi, il test di uguaglianza tra predicati. Siano $ (C \; \tau_1 \; ... \; \tau_n) $ e
$ (C' \; \tau_1' \; ... \; \tau_n') $ due constraints, essi sono uguali se:
\[ C = C' \wedge \forall i \in \{1, ..., n\}. \; \tau_i = \tau_i' \]
Ora possiamo definire il test uguaglianza per i tipi qualificati. Siano $ (P_1, ..., P_n \Rightarrow \tau) $ e
$ (Q_1, ..., Q_m \Rightarrow \tau') $ due tipi qualificati, essi sono uguali se:
\begin{enumerate}
    \item $ \tau = \tau' $
    \item $ \forall i \in \{1, ..., n\}. \; \exists j \in \{1, ..., m\}. \; P_i = Q_j $
    \item $ \forall j \in \{1, ..., m\}. \; \exists i \in \{1, ..., n\}. \; Q_j = P_i $
\end{enumerate}
Ora rimangono gli schemi di tipo. A differenza dei mono-tipi, qui le variabili possono essere quantificate, quindi non
basterebbe più definire il test di uguaglianza analizzando solamente i termini del mono-tipo. Nel codice sorgente in
\texttt{Compiler.Ast.Typed}, non vi è alcuna definizione di uguaglianza tra schemi di tipo poiché questa non è necessaria
a nessun'altra operazione.

\subsection{Polimorfismo ad-hoc}
langgg supporta il polimorfismo ad hoc esponendo costrutti chiamati \textit{proprietà}, le quali sono concettualmente
molto simili alle \textit{type classes} di Haskell. Ecco un esempio di definizione di proprietà in langgg:
\begin{lstlisting}
property Eq a =
    val (==) : a -> a -> Bool
    val (/=) : a -> a -> Bool
;;
\end{lstlisting}
Questo pezzo di codice produce idealmente due token corrispondenti ai metodi \texttt{(==)} e \texttt{(/=)},
i quali possono essere istanziati attraverso il meccanismo delle istanze che verrà presentato prossimamente. I token
appena costruiti avranno le seguenti firme:
\begin{lstlisting}
val (==) : Eq a => a -> a -> Bool
val (/=) : Eq a => a -> a -> Bool
\end{lstlisting}
Si noti come le firme effettive non siano le stesse di quelle date dall'utente, le quali risultano incomplete.
All'inizio della dicitura dei tipi effettivi, si può notare un \textit{constraint} o \textit{predicato} seguito da una
freccia, la quale è intesa come implicazione. I tipi dei metodi di proprietà avranno quindi la forma:
    \[ Pred(\overline{\alpha}) \Rightarrow ty(\overline{\alpha}) \]
dove \textit{Pred} è un predicato e \textit{ty} è un tipo qualunque sulle variabili $ \overline{\alpha} $.
Il polimorfismo ad hoc viene, dunque, supportato attraverso i predicati, i quali rappresentano delle ipotesi aggiuntive
sui tipi. L'istanziazione di una proprietà consiste nel dichiarare dei tipi che fungeranno da "caso particolare" per i
metodi di proprietà (i "modelli"), ad esempio, riprendendo la proprietà \texttt{Eq} del precedente pezzo di codice,
in langgg possiamo definire un'istanza del tipo:
\begin{lstlisting}
instance Eq Char =
    <implementazioni>
;;
\end{lstlisting}
I dettagli implementativi al momento sono omessi. In ogni caso, i metodi di istanza avranno le seguenti firme:
\begin{lstlisting}
val (==) : Char -> Char -> Bool
val (/=) : Char -> Char -> Bool
\end{lstlisting}
Si può notare come il predicato su \texttt{Eq} non esiste più.

\subsubsection{Estensioni del polimorfismo ad hoc}
Il meccanismo di polimorfismo ad hoc precedentemente presentato incontra numerose estensioni in langgg, alcune delle
quali impongono condizioni aggiuntive sul programma.

\subparagraph{Numero arbitrario di argomenti delle proprietà}
Nell'esempio con \texttt{Eq}, la proprietà possedeva soltanto un argomento, tuttavia, è possibile definire proprietà
con un numero arbitrario (ma fissato alla definizione) di argomenti ad esempio:
\begin{lstlisting}
property Stream s t =
    val new : t -> s t
    val yield : t -> s t -> s t
    val consume : s t -> (s t, t)
;;
\end{lstlisting}
Si noti che \texttt{Stream} ha 2 argomenti.

\subparagraph{Constraints sulle proprietà}
\`E possibile definire in langgg una proprietà della forma:
\begin{lstlisting}
property C1 a Char => C2 a =
    <metodi>
;;
\end{lstlisting}
Questo tipo di definizione consiste nell'aggiungere il predicato \texttt{C1 a} alla proprietà \texttt{C2 a}.
Semanticamente, questo impone la seguente condizione sul programma:
    \[ \forall type . \; \exists inst(C2, type) \Longrightarrow \exists inst(C1, type, Char) \]
dove $ inst(C, \overline{t}) $ è, banalmente, l'instanza con nome di proprietà $ C $ e tipi $ \overline{t} $. \`E
possibile aggiungere più di un predicato alla stessa proprietà.

\subparagraph{Polimorfismo higher-kinded}
Riprendiamo il precedente esempio con \texttt{Stream}:
\begin{lstlisting}
property Stream s t =
    val new : t -> s t
    val yield : t -> s t -> s t
    val consume : s t -> (s t, t)
;;
\end{lstlisting}
Gli argomenti di una proprietà sono sempre variabili di tipo. Come è già stato menzionato in precedenza, langgg supporta
tipi di kind arbitrari; gli argomenti delle proprietà non fanno eccezione da questo punto di vista, infatti, nell'esempio
si può notare come la variabile \textit{s} abbia kind (utilizzando la rappresentazione dei kind di Haskell):
    \[ * \mapsto * \]
mentre la variabile \textit{t} ha kind:
   \[ * \]

\subparagraph{Argomenti arbitrari delle istanze}
Gli argomenti di un'istanza possono essere tipi di forma arbitraria, ad esempio:
\begin{lstlisting}
property C x y =
    <metodi>
;;

instance C a (Tuple3 a (T b) (m b)) =
    <implementazioni>
;;
\end{lstlisting}
Confrontando il precedente codice langgg con il seguente codice Haskell che può considerarsi quanto meno concettualmente
equivalente:
\begin{lstlisting}
{-# LANGUAGE MultiParamTypeClasses #-}

class C x y where
    <metodi>

instance C a (a, T b, m b) where
    <implementazioni>
\end{lstlisting}
si ha che, compilando questo programma Haskell soltanto con l'estensione all'inizio del codice, si incorrerà nel
seguente messaggio d'errore da parte del compilatore:
\begin{lstlisting}
Illegal instance declaration for 'C a (a, T b, m b)'
(All instance types must be of the form (T a1 ... an)
where a1 ... an are *distinct type variables*,
and each type variable appears at most once in the instance head.
Use FlexibleInstances if you want to disable this.)
\end{lstlisting}
Come suggerisce l'ultima riga del messaggio d'errore, per definire un'istanza del genere in un programma Haskell è
necessario utilizzare l'estensione \texttt{FlexibleInstances} [7]. In langgg, a differenza di Haskell, quel tipo di instanza
è accettato di default. Il seguente vincolo sulla forma dei tipi riportato nel messaggio d'errore viene dunque rilassato:
    \[ TyCon(\overline{\alpha}) \]
dove $ \overline{\alpha} $ sono variabili di tipo distinte e $ TyCon $ è un costruttore di tipo. In seguito, verrà
mostrato quali vincoli che riguardano i constraints e le istanze possono essere rilassati e con quali conseguenze.

\subparagraph{Constraints sulle istanze}
L'utente ha anche la facoltà di definire istanze con uno o più predicati, ad esempio:
\begin{lstlisting}
instance Monad m => Stream m String =
    <implementazioni>
;;
\end{lstlisting}
questo tipo di definizione genererà metodi di istanza con le seguenti firme:
\begin{lstlisting}
val new : Monad m => String -> m String
val yield : Monad m => String -> m String -> m String
val consume : Monad m => m String -> (m String, String)
\end{lstlisting}
Il constraint sull'istanza viene quindi applicato al tipo dei metodi di istanza.

\subsubsection{Condizioni sui constraint}
I constraints rappresentano un modo per "restringere" i possibili tipi che possono istanziare una o più variabili di
tipo. I tipi nelle firme delle variabili (nonché i type-hinting) possono possedere zero o più constraint; lo stesso
vale, come viene mostrato nei paragrafi precedenti, anche per le proprietà e per le istanze. Esistono, però, dei
vincoli sulla forma dei constraint. Come viene mostrato in [8] e [9], esistono condizioni (di Paterson) sufficienti
affinché l'algoritmo di risoluzione delle istanze di Haskell termini. Prima di presentarle, diamo alcune notazioni:
\newline Dato un costrutto $ K $ in cui compaiono dei constraints $ C \Rightarrow D $, definiamo
\begin{itemize}
    \item $ C $ come \textit{Contesto} del costrutto $ K $;
    \item $ H $ come \textit{Testa} del costrutto $ K $.
\end{itemize}
Ora presentiamo le condizioni di Paterson:
\begin{itemize}
    \item il contesto \textit{C} di una dichiarazione di type-class può menzionare solamente variabili di tipo e
    le variabili di tipo sono distinte in ogni singolo constraint in \textit{C};
    \item Per ogni dichiarazione di istanza $ C \Rightarrow TyCl \; t_1 ... t_n $, nessuna variabile di tipo ha
    più occorrenze nel contesto $ C $ rispetto alla testa $ TyCl \; t_1 ... t_n $.
    \item Per ogni dichiarazione di istanza $ C \Rightarrow TyCl \; t_1 ... t_n $, ogni constraint nel contesto
    $ C $ ha meno costruttori e variabili di tipo (presi insieme e contando le ripetizioni) rispetto alla testa
    $ TyCl \; t_1 ... t_n $.
    \item Per ogni due dichiarazioni di istanze $ C \Rightarrow TyCl \; t_1 ... t_n $,
    $ C' \Rightarrow TyCl \; t_1' ... t_n' $, non deve esistere una sostituzione $ \varphi $ tale che:
    $ \varphi(t_1) = \varphi(t_1') $, ..., $ \varphi(t_n) = \varphi(t_n') $.
\end{itemize}
Informalmente, l'obiettivo è avere sempre dei contesti più "piccoli" rispetto alle teste. La politica di langgg è
simile e applica i seguenti vincoli:
\begin{enumerate}
    \item Per ogni constraint $ c $, $ c $ deve contenere almeno una variabile di tipo, ovvero:
    \[ \forall \; constraint =: c.\; \exists \; tyvar =: v.\; v \in c \]
    \item Per ogni istanza $ ctx \Rightarrow h $, il numero di occorrenze di ogni singola variabile $ v $ nel contesto
    $ ctx $ deve essere minore o uguale rispetto al numero di occorrenze di $ v $ nella testa $ h $, ovvero:
    \[ \forall \; instance =: (ctx \Rightarrow h).\; \forall \; tyvar =: v.\; v \in instance
    \Longrightarrow occ(v, ctx) \leq occ(v, h) \]
    \item Per ogni istanza $ ctx \Rightarrow h $, le variabili di tipo di $ ctx $ devono essere innestate in meno
    costruttori di tipo rispetto alle variabili di tipo di $ h $ (contando le variabili tutte insieme), ovvero: \newline
    \[ \forall \; instance =: (ctx \Rightarrow h).\; countWrapVars(ctx) < countWrapVars(h) \]
\end{enumerate}
dove $ occ(v, k) $ è la funzione che conta le occorrenze della variabile di tipo $ v $ nel costrutto $ k $ e
$ countWrapVars(k) $ è la funzione che conta in quanti costruttori di tipo nel costrutto $ k $ sono innestate le
variabili di tipo (sommando il risultato per tutte le variabili di tipo incontrate). Il codice che implementa questi
controlli risiede nel modulo \texttt{Compiler.Constraints.Check}.

\subsection{Kind-inference}
La kind-inference in langgg è necessaria per poter costruire i token che rappresentano i costruttori di tipi. Prima
di entrare nei dettagli, è bene fare alcune precisazioni sui kind. Innanzitutto, langgg non espone all'utente alcuna
sintassi per manipolare i kind o per scrivere espressioni con i kind. Come conseguenza, la definizione di kind in
langgg (cfr. Tipi higher-kinded) non viene estesa con una definizione di kind polimorfica. Questo non avviene per i tipi,
infatti, la nozione di mono-tipo viene successivamente estesa con la nozione di schema di tipo, la quale introduce
il polimorfismo parametrico.

L'ipotesi fondamentale (che connoteremo con HK) su cui si basa la kind-inference è sul kind del tipo \textit{funzione}:
\[ (\mapsto) : * \mapsto * \mapsto * \]
In generale, l'inferenza di kind avviene "osservando" le definizioni dei tipi di dati algebrici, in particolare,
guardando le definizioni dei data-constructors. La definizione di un data-constructor ha la seguente forma:
\[ C \; ty_1 \; ... \; ty_n \]
Supponendo che il data-constructor $ C $ \textit{costruisca} il tipo $ T \; \alpha_1 \; ... \; \alpha_m $, allora
$ C $ avrà tipo:
\[ ty_1 \mapsto ... \mapsto ty_n \mapsto T \; \alpha_1 \; ... \; \alpha_m \]
Come conseguenza dell'ipotesi HK si ha che:
\[ \forall i \in \{ 1, ..., n \}. \; ty_i : * \]
e:
\[ T \; \alpha_1 \; ... \; \alpha_m : * \]
Si ricordi che le variabili di tipo che compaiono nei tipi $ ty_i $ sono legate ai binders
$ \alpha_1 \; ... \; \alpha_m $. Una volta che tutti i data-constructors sono stati visitati, i kind delle variabili di
tipo saranno noti, quindi si può inferire il kind del costruttore di tipo $ T $. Durante la kind-inference viene
utilizzato un ambiente di tipizzazione (che connoteremo con $ \Delta $) in cui vengono salvati le coppie della forma:
\[ < tipo, kind > \]
dove il $ tipo $ è il nome di un qualsiasi tipo che compare senza parametri; sono comprese le variabili di tipo.
In questo contesto, i nomi dei tipi non hanno uno \textit{scope} in quanto è garantito che non ci siano tipi diversi con
nomi di uguali nel programma.
Questa condizione è banalmente garantita per i tipi concreti in quanto non è possibile definire tipi molteplici con nomi
uguali, mentre, per quanto riguarda le variabili di tipo, il renaming
delle variabili di tipo (cfr. Renaming delle variabili di tipo) garantisce la condizione. Inoltre, la sintassi degli
identificatori garantisce che non vi siano variabili di tipo e tipi concreti con nomi uguali.
In generale, vengono visitate tutte le definizioni di data-constructors nelle definizioni di tipo, seguendo queste regole:
\begin{itemize}
    \item se un tipo $ ty_i $ ha la forma di variabile di tipo $ \alpha_j $, per un qualche $ j \in {1, ..., m} $, allora
    per HK, $ \alpha_j : * $;
    \item se un tipo $ ty_i $ ha come testa un tipo concreto $ T' $ che esiste già in $ \Delta $, allora si possono
    inferire i kind degli argomenti di $ ty_i $; per i tipi che esistono già in $ \Delta $ e che non hanno una variabile
    di kind come kind associato, viene effettuato il kind-check, mentre per i tipi non presenti in $ \Delta $ o che
    hanno una variabile di kind come kind associato, vengono inseriti in $ \Delta $ con il nuovo kind inferito. Infine,
    deve valere che $ ty_i : * $;
    \item se un tipo $ ty_i $ ha come testa un tipo concreto $ T' $ che non esiste in $ \Delta $, allora si inferiscono
    i kind degli argomenti di $ ty_i $, senza effettuare la fase di kind-check. Infine, $ T' $ viene inserito in
    $ \Delta $ con kind $ K_1 \mapsto ... \mapsto K_r $, dove $ K_1, ..., K_r $ sono i kind degli argomenti applicati a
    $ T' $;
    \item se un tipo $ ty_i $ ha come testa una variabile di tipo $ m $, deve valere che $ m $ non compare come suo
    argomento diretto. Nel caso non valga la condizione, il programma viene respinto con un errore. Poi l'inferenza
    procede come per i tipi concreti;
    \item se un tipo $ arg $ compare come argomento i-esimo di un costruttore di tipo $ tycon $, se $ tycon \in \Delta $,
    allora sia $ K $ il suo kind, il kind inferito di $ arg $ sarà quello dell'i-esimo argomento di $ K $. Se
    $ tycon \notin \Delta $, allora viene creata una variabile di kind $ \kappa $ e in $ \Delta $ viene inserito il
    binding $ < arg, \kappa > $;
    \item le variabili di tipo che compaiono soltanto nei binders avranno kind $ * $;
    \item dopo che tutte le definizioni di tipo sono state visitate, le variabili di tipo in $ \Delta $ che hanno
    variabili di kind come kind associato, acquisiscono il kind $ * $;
    \item ogni qual volta un tipo che ha una variabile di kind $ \kappa $ come kind associato acquisisce un nuovo kind
    $ K $, le occorrenze di $ \kappa $ in $ \Delta $ come variabili libere devono essere $ specializzate $ con $ K $.
\end{itemize}

\subsection{Data constructors}
Una delle tabelle di simboli che appare nel modulo \texttt{Compiler.Types.Table} è \texttt{DataConsTable}. In essa
vengono salvati i \textit{data constructor} associati ai loro tipi. I costruttori - che nell'AST vengono rappresentati
dal token \texttt{ADTConstructor} - vengono trasformati nel token \texttt{NotedVal} presente nel modulo
\texttt{Compiler.Ast.Typed} che consiste nella seguente coppia:
\[ < dataConRep, type > \]
dove $ dataConRep $ è la rappresentazione sotto forma di stringa del data constructor, mentre $ ty $ è il tipo del
costruttore. Per quanto riguarda il tipo del costruttore, prendiamo in considerazione il seguente esempio di codice
langgg:
\begin{lstlisting}
type Foo x y = Bar Int (M x) y
\end{lstlisting}
In questa definizione di tipo di dato algebrico, vi è un solo costruttore: \texttt{Bar}. Il suo tipo è il seguente:
\[ \forall x, y. \; Int \mapsto M \; x \mapsto y \mapsto Foo \; x \; y \]
Il tipo di ritorno è sempre dato dalla definizione del tipo. Il modulo che si occupa di trasformare i token
\texttt{ADTConstructor} in \texttt{NotedVal} e aggiungerli nella tabella \texttt{DataConsTable} è
\texttt{Compiler.Types.Builder.Cons}.

\subsection{Constraint constructors}
Dopo il check sui constraints e la generazione di data constructors, il compilatore si occupa di generare i cosiddetti
"\textit{constraint constructors}", ovvero token che fungono da modelli per la costruzione di predicati. Il modulo
che li genera è \texttt{Compiler.Types.Tables}; esso prende in input i token dell'AST che rappresentano le proprietà
(\texttt{Interface}) e da essi costruisce i token tipati \texttt{LangNewConstraint} e li inserisce nella tabella
\texttt{ConstraintsTable}. \`E compito di questo modulo controllare che le classi non formino cicli tra loro, ad
esempio, il seguente programma viene rifiutato dal compilatore:
\begin{lstlisting}
property Bar (M a) Int => Foo a =
    <metodi>
;;

property Foo (K x Char) => Bar x y =
    <metodi>
;;
\end{lstlisting}
Si noti come i vincoli sui constraints vengano tutti rispettati.

\subsection{Costruzione delle istanze}
Nel modulo \texttt{Compiler.Types.Builder.Instances} vengono create le seguenti 3 tabelle:
\begin{itemize}
    \item \texttt{InstsTable}, ovvero la tabella che contiene i bindings (sotto forma di token dell'AST) delle istanze;
    \item \texttt{PropMethodsTable}, ovvero la tabella che contiene i metodi delle proprietà sotto forma di token
    tipati;
    \item \texttt{ImplTable}, ovvero la tabella che contiene i constraints che derivano dalle istanze definite
    dall'utente;
\end{itemize}
Per quanto riguarda l'ultima tabella, data la definizione di un'istanza:
\begin{lstlisting}
instance Eq Char =
    <implementazioni>
;;
\end{lstlisting}
la testa \texttt{Eq Char} ha la forma di un constraint, nonostante non rispetti uno dei vincoli sui constraints (un
constraint deve avere almeno una variabile di tipo), infatti l'istanza viene salvata sotto forma di constraint
(\texttt{LangSpecConstraint}).
Le tre tabelle vengono create in un unico modulo per una questione di efficienza (i token delle istanze vengono visitati
una sola volta). Inoltre, in questo
modulo viene effettuato il controllo del vincolo sull'esistenza dei predicati delle proprietà (cfr. paragrafo "constraints
delle proprietà" in "Estensioni del polimorfismo ad hoc"). Prima di inserire i bindings delle istanze nella
tabella \texttt{InstsTable}, viene eseguita una fase di desugaring su di essi. Il problema nasce dal fatto che, data
una proprietà, può esistere un numero arbitrario di istanze e con esse, un numero arbitrario di metodi con lo stesso
nome, perciò è necessario identificare univocamente i metodi di ogni singoli istanza. Il modulo
\texttt{Compiler.Desugar.Names} si occupa di, dato il nome di una variabile e una sequenza di constraints (gli argomenti
di un'istanza), creare un identificatore univoco che non può essere uguale a nessun altro identificatore nel programma.

\subsection{Preparazione alla type-inference}
Prima di effettuare la type-inference è necessario fare alcune considerazioni ed effettuare alcune computazioni.
Innazitutto, il type-system di riferimento è \textit{Hindley-Milner} (o Damas-Hindley-Milner, in seguito lo indicheremo con HM)
con alcune estensioni che riguardano il costrutto del pattern matching, l'inferenza di definizioni ricorsive e la
risoluzione delle istanze. Il modulo che si occupa della preparazione alla type-inference è \texttt{Compiler.Types.Prepare}
Introdurremo il type-system in modo preciso prossimamente (cfr. Type-inference), tuttavia, ora presentiamo alcune
caratteristiche del type-system che è bene conoscere come premessa alla preparazione della type-inference.
Innanzitutto, HM viene descritto formalmente da un insieme di \textit{regole} le quali si occupano di "tipare"
espressioni di un formalismo fissato: una versione del lambda-calcolo estesa con un costrutto di espressioni "let..in".
Una delle regole è quella sull'inferenza del tipo di un simbolo (la regola la chiameremo chiamiamo \textbf{Var})
la quale, informalmente, sostiene che: data l'ipotesi di un simbolo $ x $ con schema di tipo $ \sigma $ nel contesto di
un ambiente di tipizzazione $ \Gamma $ e $ \tau $ l'istanziazione del tipo $ \sigma $, si conclude che il simbolo
$ x $ ha tipo $ \tau $.
Un'altra regola utile è quella
sull'inferenza delle definizioni mutualmente ricorsive. In particolare, essa sostiene che, l'inferenza di definizioni
mutualmente ricorsive avviene in "gruppo", ovvero un insieme $ {f_1, ..., f_n} $ di simboli mutualmente ricorsivi
tra loro vengono considerati nel loro insieme e non singolarmente (regola \textbf{Rec}) (cfr. Ricorsione).
Queste regole serviranno come premessa ad alcune fasi nella preparazione della type inference.

\subsubsection{Bindings delle istanze}
\`E bene notare come in Core non esista un costrutto particolare per i bindings delle istanze, inoltre, il
costrutto di GHC che rappresenta le istanze (\texttt{ClsInst}) non porta con sè informazioni che riguardano i
bindings [10].
Perciò, è in un qualche modo necessario gestire i bindings delle istanze. La politica del compilatore è quella di
aggiungerli nell'insieme di bindings da dare in input all'algoritmo di type-inference. La presenza di bindings con lo
stesso identificatore è stata già risolta (cfr. Costruzione delle istanze e dispatch statico).

\subsubsection{Simboli innestati univoci}
Prima della type-inference, vengono visitate tutte le espressioni dei bindings e vengono creati nuovi identificatori
per ogni definizione innestata di variabile (costrutto \texttt{let..in}). I nuovi identificatori sono resi univoci in
tutto il programma. Il modulo che crea e garantisce che i nuovi identificatori sono univoci è
\texttt{Compiler.Desugar.Names}. La proprietà di unicità degli identificatori innestati è molto importante, vedremo in
seguito il motivo (cfr. type-inference).

\subsubsection{Clusters di definizioni mutualmente ricorsive}
Come è stato già menzionato precedentemente, le definizioni mutualmente ricorsive devono essere considerate come insiemi
e non singolarmente. Perciò è necessario distinguere due tipi di bindings, quelli mutualmente ricorsivi e i restanti:
\begin{lstlisting}
data RawBinding =
      RawNonRec (Raw.SDUnion With.ProgState)
    | RawRec [Raw.SDUnion With.ProgState]
\end{lstlisting}
La precedente definizione è nel modulo \texttt{Compiler.Types.Prepare.Lib} e divide i bindings dell'AST in bindings
non ricorsivi e bindings mutualmente ricorsivi. Rimane, quindi, da dividere i bindings del programma. Prima di presentare
l'algoritmo, bisogna effettuare una precisazione molto importante. In precedenza, abbiamo aggiunto i bindings delle
istanze alla lista dei bindings del programma, cambiando, però, gli identificatori, rendendoli univoci all'interno del
programma. Per dividere riconoscere i bindings mutualmente ricorsivi è necessaria una funzione che calcoli le dipendenze
di una definizione. Nelle espressioni, in generale, possono essere menzionati i metodi delle proprietà, ma non possono,
in alcun modo, essere menzionati i metodi delle istanze. Inoltre, il tipo dei metodi delle proprietà è sempre noto a
priori e non è possibile, prima della type-inference, inferire le istanze (cfr. static dispatch) giuste dei metodi.
Nel calcolo delle dipendenze, i metodi di proprietà possono, quindi, essere esclusi. La funzione \textit{depsOf}, la quale
prende in input un binding \texttt{b} dell'AST e la tabella \texttt{PropMethodsTable}, calcolerà le dipendenze di
\texttt{b}, escludendo le dipendenze che provengono dalla tabella dei metodi di proprietà. L'insieme dei bindings di
un programma può essere visto come un grafo orientato $ G $ in cui:
\begin{itemize}
    \item i nodi sono i bindings;
    \item gli archi sono le dipendenze di un binding.
\end{itemize}
Il problema di trovare i clusters di definizioni mutualmente ricorsive corrisponde a trovare le \textit{componenti
fortemente connesse} del grafo $ G $. Il modulo che implementa l'algoritmo è \texttt{Compiler.Types.Prepare.Recursion}
e utilizza la libreria \texttt{Data.Graph}.

\subsubsection{Sorting dei bindings}
A questo punto, abbiamo una lista di \texttt{RawBinding}. Possiamo fare la seguente osservazione: i bindings possono
essere visti come un grafo orientato aciclico $ G $, in cui, come prima:
\begin{itemize}
    \item i nodi sono i bindings;
    \item gli archi sono le dipendenze di un binding.
\end{itemize}
L'unica differenza è sull'aciclicità del grafo. Questa proprietà è garantita dal seguente fatto: se esistesse un ciclo
in $ G $, allora l'algoritmo di raggruppamento dei clusters l'avrebbe trovato e avrebbe creato un cluster di definizioni
mutualmente ricorsive tra loro. La regola di inferenza \textbf{Var} impone che si conosca sempre lo schema di tipo di
un simbolo, perciò, è necessario, prima di effettuare la type-inference, ordinare i bindings in base alle loro dipendenze.
L'ordinamento viene effettuato nel modulo \texttt{Compiler.Types.Prepare.Sort} e l'algoritmo utilizzato è un'estensione
dell'insertion-sort, dove, la nozione di ordinamento è data dalle dipendenze dei bindings, ovvero: \newline
Siano $ b $ e $ b' $ due bindings, si ha che:
\begin{itemize}
    \item $ b > b' $, se $ b' $ è una dipendenza diretta di $ b $ oppure esistono dei bindings $ c_1, ..., c_n $ tali che:
    \newline $ (b > c_1) \wedge (c_1 > c_2) \wedge ... \wedge (c_{n-1} > c_n) \wedge (c_n > b') $;
    \item $ b = b' $, se la componente di $ b $ in $ G $ non è raggiungibile da nessun nodo della componente $ b' $.
\end{itemize}
la proprietà di transitività di questa nozione di ordinamento è garantita dall'assenza di cicli all'interno del grafo.
Di seguito vi è l'algoritmo di sorting di una singola componente del grafo:
\begin{lstlisting}
sortComponent(bindings, remaining, component):
    match bindings, remaining with
        [], [] -> component
        [], (_ : _) -> sortComponent remaining [] component
        (b : t), _ ->
            let (inserted, component') = tryInsert b component in
                if inserted
                then sortComponent t remaining component'
                else sortComponent t (addTail b remaining) component 
\end{lstlisting}
L'algoritmo prende in input i bindings di una componente, i bindings rimanenti e la componente già ordinata. Ora
presentiamo l'algoritmo di inserzione di un binding in una componente già ordinata:
\begin{lstlisting}
tryInsert(binding, component):
    match component with
        [] -> [component]
        (b : t) ->
            if b in depsOf(binding)
            then tryInsert binding t
            else if binding in depsOf(b)
            then (True, binding : component)
            else (False, component)
\end{lstlisting}
Ora abbiamo implementato una versione alternativa dell'insertion sort in cui gli inserimenti vengono effettuati
solamente se si è a conoscenza che il binding da inserire fa parte delle dipendenze dirette di un altro binding già
inserito nella componente. Inoltre, se non vi sono abbastanza informazioni per inserire un binding in una componente,
il suo inserimento viene "rimandato" (viene inserito nei bindings rimanenti) e verrà effettuato in un'iterazione
successiva. Il codice si trova nel modulo \texttt{Compiler.Types.Prepare.Sort}.

Dopo l'ordinamento dei bindings, è garantito che, ogni qual volta verrà inferito un binding, la premessa
della regola \textbf{Var} sia vera.

\subsection{Type-inference}
langgg permette all'utente di indicare o non indicare il tipo di una variabile. Nel caso non venga indicato, il
compilatore dovrà inferire il tipo della variabile definita, quindi dovrà fare una "scelta". Per farlo, dovrà prima
inferire il tipo dell'espressione legata alla variabile, ad esempio:
\begin{lstlisting}
type Maybe a = Nothing | Just a
let x = Nothing
\end{lstlisting}
Si può affermare che l'espressione legata alla variabile $ x $ abbia tipo $ Maybe \; \alpha $. Ora, il compilatore
deve scegliere un tipo da sostituire alla variabile di tipo $ \alpha $. Se la scelta fosse arbitraria, ad esempio
$ Int $, l'utilizzo di $ x $ sarebbe limitato solo a un tipo ovvero $ Maybe \; Int $. Un'altra possibile soluzione
potrebbe essere associare a $ x $ il tipo $ Maybe \; a $. Tuttavia, anche questa scelta risulterebbe limitante in quanto,
come è stato già esposto nella sezione sul type-system (cfr. Schemi di tipi), la variabile di tipo $ \alpha $
denoterebbe un tipo fissato non ancora conosciuto. L'introduzione di uno schema di tipo rappresenta una scelta
"più generale":
\[ x : \forall \alpha. \; Maybe \; \alpha \]
In questo caso, $ x $ può avere molteplici tipi.

\subsubsection{Damas-Hindley-Milner}
Come è stato già menzionato precedentemente, il sistema di tipi di langgg si basa sul type-system di Damas-Hindley-Milner.
L'algoritmo di inferenza su cui si basa inferisce il tipo più \textit{generale} possibile. Tuttavia, nella letteratura,
vengono presentati due algoritmi principali, ma nella pratica, solo uno di essi è implementato dai compilatori dei
linguaggi di programmazione. Verranno entrambi menzionati, ma solo le regole di inferenza dell'algoritmo effettivamente
implementato verranno esposte:
\begin{itemize}
    \item \textit{Algoritmo W}, è l'algoritmo effettivamente utilizzato dalle implementazioni, in quanto tiene traccia
    delle sostituzioni generate dalle eventuali unificazioni (cfr. Unificazione) e le applica all'ambiente di
    tipizzazione. Proprio a causa della gestione delle sostituzioni, la complessità computazionale dell'algoritmo
    nel caso peggiore è esponenziale.
    \item \textit{Algoritmo J}, è un algoritmo più efficiente di \textit{W} (lineare nella lunghezza dell'espressione),
    tuttavia, esso non gestisce le sostituzioni, o almeno, esse vengono considerate come side-effects. Viene
    presentato in letteratura come alternativa efficiente a \textit{W}.
\end{itemize}

\subsubsection{Regole di inferenza}
Come è stato già anticipato, HM viene presentato sotto forma di \textit{regole}, le quali hanno la seguente forma:
\newline
\[ \infer[\textbf{Regola}]{Conclusione}{Premesse} \]
dove le premesse sono un insieme di \textit{giudizi} e \textit{predicati} e la conclusione è un \textit{giudizio}. Un
\textit{giudizio} è un'affermazione sul tipo di un simbolo, ad esempio:
\[ x : \sigma \]
afferma che il simbolo $ x $ ha tipo $ \tau $. HM necessita, inoltre, di un modo per "tener traccia" dei giudizi,
ovvero un modo per accoppiare un simbolo con un tipo. Introduciamo, quindi,
un altro componente: il \textit{contesto} o \textit{ambiente di tipizzazione}. In generale, i giudizi terranno conto
del contesto, infatti avranno la seguente forma:
\[ \Gamma \vdash_W x : \sigma \]
questa scritta afferma che sotto le ipotesi in $ \Gamma $, il token $ x $ ha tipo $ \sigma $.
Ora verranno elencate le regole di inferenza:

\paragraph{Inferenza di variabile}
Prima di presentare di la regola d'inferenza delle variabili è necessario due ulteriori regole. La prima è la
regola di specializzazione e introduce una nozione d'ordine parziale tra tipi:
\[ \infer[\textbf{Spec}]{\forall \alpha_1 ... \forall \alpha_n. \; \tau \sqsubseteq \forall \beta_1 ... \beta_m. \; \tau'}{\tau' = \{ \alpha_i \mapsto \tau_i \}\tau & \beta_i \notin free(\forall \alpha_1 ... \forall \alpha_n . \; \tau)} \]
Questa regola sostiene che se esiste una sostituzione $ S = \{ \alpha_i \mapsto \tau_i \} $ tale che $ \tau' = S \tau $,
allora la versione "quantificata" dei mono-tipi $ \tau $ e $ \tau' $ rispetta la relazione d'ordine parziale
$ \sqsubseteq $. Nella premessa vi è un'ulteriore condizione che afferma che le variabili quantificate di $ \tau' $
non devono apparire come variabili libere in $ \forall \alpha_1 ... \alpha_n. \; \tau $; questo perché le variabili
non legate da un quantificatore (quindi libere) non devono essere sostituite, bensì devono essere trattate come costanti.
All'interno del codice, i token tipati che implementano la type-class \texttt{SpecType} in \texttt{Compiler.Ast.Typed}
permettono l'applicazione di una sostituzione al loro tipo.
La prossima regola rappresenta l'algoritmo di instanziazione:
\[ \infer[\textbf{Inst}]{\Gamma \vdash_W e : \tau}{\Gamma \vdash_W e : \sigma & \sigma \sqsubseteq \tau} \]
Questa regola è utile per istanziare uno schema di tipo in mono-tipo. \`E chiaro che è necessario tener conto delle
eventuali variabili libere nello schema di tipo $ \sigma $, ma queste vengono gestite dalla regola di specializzazione.
Ora, possiamo esporre la regola di inferenza di una variabile, la quale è stata già introdotta in precedenza, seppur
in maniera informale.
\[ \infer[\textbf{Var}]{\Gamma \vdash_W x : \tau, \; \emptyset}{x : \sigma \in \Gamma & \tau = inst(\sigma)} \]
\`E doveroso notare che nella conclusione, oltre al giudizio sul tipo della variabile, vi è un altro valore in "output",
ovvero le eventuali sostituzioni generate dall'inferenza dei costrutti coinvolti. In questo non vi è alcuna sostituzione.

\paragraph{Inferenza di applicazione}
La prossima regola serve per inferire l'applicazione di due espressioni:
\[ \infer[\textbf{App}]{\Gamma \vdash_W e_0 \; e_1 : S_2\tau', \; S_2S_1S_0}{\Gamma \vdash_W e_0 : \tau_0, \; S_0 & S_0\Gamma \vdash_W e_1 : \tau_1, \; S_1 & \tau' = newvar & S_2 = mgu(S_1\tau_0, \tau1 \mapsto \tau')} \]
Vi sono due nuovi operatori:
\begin{itemize}
    \item $ newvar $, il quale ritorna una nuova variabile di tipo $ \alpha $ tale che $ \alpha \notin free(\Gamma) $;
    \item $ mgu $, il quale rappresenta l'algoritmo di unificazione che verrà presentato dettagliatamente nel prossimo
    paragrafo. Tralasciando, quindi, i dettagli implementativi, tale operatore serve per trovare il tipo più generale.
    Come risultato, fornisce una sostituzione $ S_2 $, la quale viene successivamente applicata al mono-tipo $ \tau' $
    per ottenere il tipo di ritorno del costrutto di applicazione.
\end{itemize}
Si noti che, a differenza di \textbf{Var}, in questa regola vi sono tre sostituzioni in output
\begin{itemize}
    \item $ S_0 $ derivante dall'inferenza della prima espressione;
    \item $ S_1 $ derivante dall'inferenza della seconda espressione;
    \item $ S_2 $ derivante dall'esecuzione dell'algoritmo di unificazione tra tipi;
\end{itemize}
Nell'inferenza di $ e_1 $, l'ambiente di tipizzazione di riferimento è $ \Gamma $ a cui viene applicata la sostituzione
$ S_0 $. Una possibile ottimizzazione è la seguente:
\[ \infer[\textbf{App}]{S_0\Gamma \vdash_W e_0 \; e_1 : S_2\tau', \; S_2S_1}{\Gamma \vdash_W e_0 : \tau_0, \; S_0 & S_0\Gamma \vdash_W e_1 : \tau_1, \; S_1 & \tau' = newvar & S_2 = mgu(S_1\tau_0, \tau1 \mapsto \tau')} \]
La sostituzione $ S_0 $ viene, quindi, spostata dall'output direttamente all'ambiente di tipizzazione. Questo per evitare
di applicare la sostituzione all'ambiente di tipizzazione due volte: una prima di inferire $ e_1 $, una dopo il risultato
di output (si presume che le sostituzioni vengano tutte applicate).

\paragraph{Unificazione}
L'unificazione è quel processo di risoluzione di equazioni tra espressioni "simboliche". In generale, i mono-tipi nel
type-system utilizzato in HM (e conseguentemente anche quello utilizzato da langgg) possono essere considerati come
termini di un'equazione. Si parla di mono-tipi, poiché l'unificazione non prevede la quantificazione delle variabili
all'interno di un'equazione. Un problema di unificazione è un insieme finito di equazioni:
\[ \{ \tau_1 = \tau_1' \; ... \; \tau_n = \tau_n' \} \]
dove $ \tau_i, \tau_i' $ sono dei mono-tipi. La soluzione di un problema di unificazione è data da una sostituzione
$ S = \{ \alpha_j \mapsto \mu_j \} $, dove $ \alpha_j $ è una variabile di tipo e $ \mu_j $ è un mono-tipo, tale che:
\[ \forall i \in \{1, ..., n\}. \; S\tau_i = S\tau_i' \]
Il codice sorgente che si occupa dell'unificazione si trova nel modulo \texttt{Compiler.Ast.Typed}, in particolare,
la funzione \texttt{rawUnify} implementa l'algoritmo di unificazione, osserviamo la sua firma:
\begin{lstlisting}
rawUnify
    :: LangHigherType a
    -> LangHigherType a
    -> IsSpecTest
    -> Either (UnificationError a) (Substitution a)
\end{lstlisting}
Notiamo subito che vi sono tre argomenti e che il tipo di ritorno è un result-type.
\begin{lstlisting}
data UnificationError a =
      UnmatchTypes (LangHigherType a) (LangHigherType a)
    | TrySwap (LangHigherType a) (LangHigherType a)
    | OccursCheck (LangHigherType a) (LangHigherType a)
    | UnmatchKinds (LangHigherType a) (LangHigherType a)

type IsSpecTest = Bool
\end{lstlisting}
Il tipo \texttt{UnificationError} possiede 4 casi d'errore e tutti prendono in input due mono-tipi:
\begin{itemize}
    \item \texttt{UnmatchTypes}, ritornato quando non vi è alcuna soluzione all'equazione tra due mono-tipi;
    \item \texttt{TrySwap}, simile a \texttt{UnmatchTypes} nella semantica, ma indica all'algoritmo di non terminare
    subito, analizzeremo approfonditamente questo caso in seguito;
    \item \texttt{OccursCheck}, ritornato quando vi è il tentativo di risolvere un problema del tipo:
        \[ \alpha = f \; ... \; \alpha \; ...  \]
    Questo tipo di equazione porterebbe come risultato un termine infinito visto che $ \alpha $ è sotto-termine di
    se stesso;
    \item \texttt{UnmatchKinds}, ritornato quando i kinds di due mono-tipi non coincidono. In questo caso, non vi
    può essere alcuna soluzione all'equazione, si pensi, ad esempio, a:
        \[ M \; \alpha = M \; Int \; \beta \]
\end{itemize}
Ora passiamo all'algoritmo vero e proprio, il quale sarà presentato mostrando direttamente soltanto una parte di esso
all'interno del codice sorgente in \texttt{rawUnify}:
\begin{lstlisting}
rawUnify monoTy monoTy' isSpecTest =
    case unification monoTy monoTy' empty of
        Left err -> Left err
        Right vvars -> Right $ elems vvars
    where
        unification lhty lhty' vvars
            | not $ sameInfrdKindOf lhty lhty' =
                Left $ UnmatchKinds lhty lhty'
            | occursCheck lhty lhty' =
                Left $ OccursCheck lhty lhty'
            | tyVarAndConcrete lhty lhty' =
                if isSpecTest
                then Left $ UnmatchTypes lhty lhty'
                else unification lhty' lhty vvars
            | bothConcrete lhty lhty' =
                if headEq lhty lhty'
                then argsUnification lhty lhty' vvars
                else Left $ UnmatchTypes lhty lhty'
            | bothTyVar lhty lhty' && argsOf lhty' > argsOf lhty =
                if isSpecTest
                then Left $ UnmatchTypes lhty lhty'
                else unification lhty' lhty vvars
            | otherwise =
                argsUnification lhty lhty' vvars

        argsUnification =
            if isSpecTest
            then unifyOnArgs
            else twoChancesUnifyArgs

        twoChancesUnifyArgs lhty lhty' vvars =
            case unifyOnArgs lhty lhty' vvars of
                Left err @ (TrySwap _ _) ->
                    case unifyOnArgs lhty' lhty vvars of
                        Left _ -> Left err
                        ok @ (Right _) -> ok
                err @ (Left _) -> err
                ok @ (Right _) -> ok

        <resto del codice>
--$ rm this when building
\end{lstlisting}
I commenti nel codice sorgente sono stati rimossi per evitare confusione. Come è stato fatto notare in precedenza,
l'algoritmo prende in input tre argomenti, di cui i primi due sono i mono-tipi sui quali si vuole calcolare l'unificazione.
Il terzo argomento è un flag booleano che denota se \texttt{rawUnify} deve essere un test di specializzazione
(\texttt{True}) o semplicemente l'algoritmo di unificazione (\texttt{False}). Un test di specializzazione è l'equivalente
dell'unificazione, ma può essere fatta in un solo "verso", ovvero se, dati due mono-tipi $ \tau $ e $ \tau' $ e data
la sostituzione di ritorno $ S = \{ \alpha_i \mapsto \tau_i \} $ conseguente alla valutazione dell'unificazione di
$ \tau $ e $ \tau' $, vale:
\[ \forall \alpha_i \in left(S). \; \alpha_i \in \tau' \]
dove con $ \alpha_i \in left(S) $ si intendono solamente le variabili di tipo nella parte sinistra di una sostituzione,
ovvero quelle che devono essere sostituite. Questo rende l'unificazione più stringente
e ciò è utile quando è richiesto che uno dei due mono-tipi non venga modifcato,
ad esempio, quando deve essere effettuato il type-check con un tipo che deriva da una annotazione di tipo (type-hinting).
Infatti, ci si aspetta che, data un'espressione con type-hinting, l'espressione abbia esattamente il tipo indicato
nell'annotazione di tipo.

L'obiettivo dell'algoritmo è riportarsi in una situazione tale che l'equazione tra i due mono-tipi ha la seguente forma:
\[ \alpha = \tau \]
dove $ \alpha $ è una variabile di tipo e $ \tau $ è un mono-tipo. Questo perché tale equazione ha la forma di una
sostituzione: $ \alpha = \tau $.
Tuttavia, nell'algoritmo, viene considerata la forma:
\[ \tau = \alpha \]
Di fatto, però, la semantica dell'unificazione non cambia; i termini dell'equazione possono essere anche invertiti,
cambiando, però, il "verso" dei controlli e delle operazioni nell'algoritmo.
Per ottenere un'equazione di quella forma vi sono due operazioni fondamentali:
\begin{itemize}
    \item \textit{swap}, in cui, se deve essere effettuata l'unificazione di due mono-tipi $ \tau $ e $ \tau' $,
    l'ordine di quest'ultimi viene scambiato. Questa operazione è utile quando il mono-tipo $ \tau' $ è più
    "specializzato" rispetto a $ \tau $. In generale, il mono-tipo più generale (o meno specializzato) possibile
    è una variabile singoletto, la quale, come è stato menzionato prima, deve stare nella parte destra dell'equazione.
    \item \textit{decompose}, in cui, se deve essere effettuata l'unificazione di due mono-tipi $ \tau $ e $ \tau' $,
    viene effettuata l'unificazione tra i vari argomenti dei due mono-tipi, raccogliendo successivamente le
    varie sostituzioni ottenute. Questa operazione è utile quando un'operazione di \textit{swap} non è vantaggiosa (ovvero
    il mono-tipo $ \tau $ è già "più specializzato" di $ \tau' $). Essa permette di considerare mono-tipi più piccoli.
\end{itemize}

Analizzando il codice, la prima chiamata viene fatta a \texttt{unification}, la quale considera varie casistiche:
\begin{enumerate}
    \item
    \begin{lstlisting}
| not $ sameInfrdKindOf lhty lhty' =
    Left $ UnmatchKinds lhty lhty'
    \end{lstlisting}
    Nel primo caso viene effettuato un controllo sui kind dei due mono-tipi. Se sono diversi, viene restituito un errore;
    \item
    \begin{lstlisting}
| occursCheck lhty lhty' =
    Left $ OccursCheck lhty lhty'
--$ rm this when building
    \end{lstlisting}
    Nel secondo caso viene effettuato un altro controllo: il cosiddetto \textit{occurs check}. Se uno dei due mono-tipi
    ha la forma di una variabile di tipo e quest'ultima compare nell'altro mono-tipo, allora viene ritornato un errore.
    \item
    \begin{lstlisting}
| tyVarAndConcrete lhty lhty' =
    if isSpecTest
    then Left $ UnmatchTypes lhty lhty'
    else unification lhty' lhty vvars
--$ rm this when building
    \end{lstlisting}
    Nel terzo caso vi è il primo controllo sulla struttura dei mono-tipi: se \texttt{lhty} ha come testa una variabile
    di tipo e \texttt{lhty'} ha come testa un tipo concreto, allora vi è un'operazione di \textit{swap}, ovvero vi è
    una chiamata ricorsiva di \texttt{unification} con gli argomenti invertiti. Se l'unificazione è \textit{strict},
    viene ritornato un errore, in quanto una variabile di tipo, per definizione, non è più specializzata di un tipo
    concreto. In questo caso, l'operazione di \textit{swap} non può essere effettuata in quanto verrebbe a meno il vincolo
    secondo il quale le variabili del primo mono-tipo (\texttt{lhty}) non devono essere sostituite.
    \item
    \begin{lstlisting}
| bothConcrete lhty lhty' =
    if headEq lhty lhty'
    then argsUnification lhty lhty' vvars
    else Left $ UnmatchTypes lhty lhty'
--$ rm this when building
    \end{lstlisting}
    Nel quarto caso viene testato se entrambi i mono-tipi hanno tipi concreti come testa; se così fosse, le teste
    devono rappresentare lo stesso tipo concreto, altrimenti viene restituito un errore, in quanto non esisterebbe
    alcuna sostituzione che permetterebbe l'uguaglianza tra i due mono-tipi. Se, invece, le teste sono uguali, allora
    viene eseguita un'azione di \textit{decompose}, ovvero vengono effettuate delle chiamate ricorsive sugli argomenti
    dei due mono-tipi. Analizzeremo la funzione \texttt{argsUnification} in seguito.
    \item
    \begin{lstlisting}
| bothTyVar lhty lhty' && argsOf lhty' > argsOf lhty =
    if isSpecTest
    then Left $ UnmatchTypes lhty lhty'
    else unification lhty' lhty vvars
--$ rm this when building
    \end{lstlisting}
    Nel quinto caso viene controllato se entrambi i mono-tipi hanno variabili di tipo come testa e se il numero di
    argomenti del secondo mono-tipo sono strettamente maggiori di quelli del primo. In questo caso, vi è un'operazione
    di \textit{swap} e la conseguente chiamata ricorsiva. Come prima, se l'unificazione è \textit{strict},
    l'operazione di \textit{swap} non può essere effettuata.
    \item
    \begin{lstlisting}
| otherwise =
    argsUnification lhty lhty' vvars
    \end{lstlisting}
    Nell'ultimo caso, vengono valutati tutti i casi restanti: quello che viene eseguito è un'azione di decompose sugli
    argomenti dei due mono-tipi.
\end{enumerate}

La funzione \texttt{argsUnification} implementa l'azione di \textit{decompose}. Per farlo, è necessario "accoppiare" gli
argomenti dei mono-tipi e questo avviene in \texttt{unifyOnArgs} che si occupa di effettuare le eventuali chiamate
ricorsive. Senza entrare nei dettagli implementativi di \texttt{unifyOnArgs}, una volta che le chiamate ricorsive sono
state effettuate, se non vi è stato alcun errore, allora l'output sarà una sequenza di sostituzioni. Tuttavia, le
sostituzioni potrebbero non essere consistenti tra loro; si osservi il seguente esempio:
\[ \tau = M \; k \; k, \; \tau' = M \; a \; b \]
dove $ M $ è un tipo concreto, mentre gli altri termini sono tutti variabili di tipo. Effettuando un'azione di
\textit{decompose}, si ottengono le seguenti sostituzioni:
\[ S_1 = \{k \mapsto a\}, \; S_2 = \{k \mapsto b\} \]
\`E chiaro che $ S_1 $ e $ S_2 $ non sono consistenti. In questi casi, \texttt{unifyOnArgs}, invece di ritornare
l'errore \texttt{UnmatchTypes}, ritorna \texttt{TrySwap}. La semantica di quest'ultimo indica che è necessario effettuare
un'azione di \textit{swap} e riprovare con \textit{decompose}. Continuando con l'esempio precedente, otterremo:
\[ S_1 = \{a \mapsto k\}, \; S_2 = \{b \mapsto k\} \]
In questo caso, $ S_1 $ e $ S_2 $ possono essere unite in un'unica sostituzione:
\[ S = \{ a \mapsto k, \; b \mapsto k \} \]
In generale, se anche il "secondo tentativo" non va a buon fine, l'algoritmo termina con un errore. La funzione che
implementa questa parte è \texttt{twoChancesUnifyArgs}. Chiaramente, come si può notare dall'implementazione di
\texttt{argsUnification}, se l'unificazione è \textit{strict}, allora il "secondo tentativo" non può essere effettuato,
in quanto vi è prima un'azione di \textit{swap} che dovrebbe essere eseguita.

\paragraph{Inferenza di lambda astrazione}
La seguente regola inferisce il tipo del costrutto di lambda-astrazione:
\[ \infer[\textbf{Abs}]{\Gamma \vdash_W \lambda x. e : S\tau \mapsto \tau', S}{\tau = newvar & \Gamma, \; x : \tau \vdash_W e : \tau', \; S} \]
Nell'inferenza di questo costrutto, l'argomento $ x $ ha come tipo una nuova variabile di tipo, alla quale viene
successivamente applicata la sostituzione derivante dall'inferenza dell'espressione $ e $.

\paragraph{Inferenza del costrutto "let..in"}
Prima di presentare la regola di inferenza, è necessario introdurre la regola di generalizzazione:
\[ \infer[\textbf{Gen}]{\Gamma \vdash_W e : \forall \alpha. \; \tau}{\Gamma \vdash_W e : \tau & \alpha \notin free(\Gamma)} \]
Questa regola quantifica le variabili di tipo che non compaiono già come variabili libere nell'ambiente di tipizzazione.
Per il costrutto \textit{let..in}, per indicare la generalizzazione, utilizzeremo la seguente notazione:
\[ \overline{\Gamma}(\tau) = \forall \overline{\alpha}. \; \tau \qquad \overline{\alpha} = free(\tau) - free(\Gamma) \]
Ora possiamo presentare la regola di inferenza del costrutto \textit{let..in}:
\[ \infer[\textbf{Let}]{\Gamma \vdash_W let \; x = e_0 \; in \; e_1 : \tau', \; S_1S_0}{\Gamma \vdash_W e_0 : \tau, \; S_0 & S_0\Gamma, x : \overline{S_0\Gamma}(\tau) \vdash_W e_1 : \tau', \; S_1} \]
\`E doveroso spiegare il motivo dell'esistenza del costrutto \textit{let..in}. Osserviamo il seguente esempio di
programma scritto nel formalismo del lambda-calcolo polimorfico:
\[ (\lambda id. \; (id \; True, id \; 42)) (\lambda x. \; x) \]
un tale programma non può essere tipato, in quanto il tipo della variabile \textit{id} non è polimorfo. Si ricordi,
infatti, che nel costrutto della lambda-astrazione, il tipo dell'argomento è un mono-tipo. Per questo motivo, è
necessario il costrutto \textit{let..in}:
\[ let \; id = \lambda x. \; x \; in \; (id \; True, id \; 42) \]
infatti, il programma appena mostrato è \textit{ben tipato}, in quanto il tipo di \textit{id} è uno schema di tipo.

\subsubsection{Implementazione delle regole di inferenza}
Il codice sorgente che implementa le regole di inferenza (e quindi la type-inference) è situato nel modulo
\texttt{Compiler.Types.Builder.Type}. Alcuni costrutti di langgg si ispirano a quelli del formalismo \textit{System-F},
tuttavia, molti vengono estesi con sintassi differenti. Ad esempio, prendendo come riferimento la definizione del token
dell'AST che rappresenta il costrutto di lambda-astrazione:
\begin{lstlisting}
newtype Lambda a = Lambda ([SymbolName a], Expression a, a)
\end{lstlisting}
si può notare come non vi sia un unico argomento, bensì, una lista. Stessa argomentazione vale per l'applicazione
di espressioni:
\begin{lstlisting}
newtype AppExpression a = AppExpr (Expression a, [Expression a], a)
\end{lstlisting}
Con le dovute accortezze, l'algoritmo di inferenza considera questi costrutti come l'equivalente di costrutti
concatenati in \textit{System-F}. Ad esempio:
\[ \texttt{lam x y z -> x + y + z} \equiv \lambda x. \; \lambda y. \; \lambda z. \; x + y + z \]

Un altra nota è necessaria sulle definizioni di simboli in langgg. Infatti, langgg permette la definizione di simboli
globali e questo "rompe" la regola di inferenza del costrutto \textit{let..in}. Questo problema viene risolto
considerando soltanto parte delle premesse del costrutto \textit{let..in}. Ad esempio:
\begin{lstlisting}
let x = e
\end{lstlisting}
come prima azione, viene inferito il tipo di \texttt{e}, sia esso il mono-tipo $ \tau $, poi viene effettuata la
generalizzazione, quindi $ x $ avrà tipo $ \forall \overline{\alpha}. \; \tau $. A questo punto, non è necessario
compiere altre azioni e il binding $ x : \forall \overline{\alpha}. \; \tau $ può essere aggiunto nell'ambiente di
tipizzazione.

Il costrutto \textit{let..in} in langgg ha nella sintassi un'altra estensione rispetto al suo omonimo in \textit{System-F},
ovvero i bindings accettano anche argomenti:
\begin{lstlisting}
let f x y = (x, y)
\end{lstlisting}
In questo caso, la politica dell'algoritmo di inferenza è trattare gli argomenti \texttt{x} e \texttt{y} come argomenti
di un costrutto di lambda-astrazione (cfr. Lambda-astrazione), quindi, si può pensare a questa equivalenza in termini
di type-inference con \textit{System-F}:
\[ \texttt{let f x y = (x, y)} \equiv let \; f = \lambda x. \; \lambda y. \; (x, y) \]

\subsection{Estensioni dell'algoritmo di inferenza}
langgg possiede numerosi costrutti che arricchiscono la sintassi di base di \textit{System-F}, inoltre, introduce
concetti nuovi come i predicati o i tipi qualificati. Di conseguenza, l'algoritmo di inferenza di tipo di \textit{System-F}
deve essere esteso.

\subsubsection{Pattern matching}
langgg possiede due costrutti diversi per il pattern matching (cfr. Caratteristiche del linguaggio). Il primo costrutto
permette "ispezionare" il valore di una singola espressione, per esempio:
\begin{lstlisting}
type Maybe a = Nothing | Just a

val f : List a -> String
let f l =
    match first l with
          Nothing -> "Error_404"
        | Just _ -> "Ok_200"
\end{lstlisting}
Il secondo costrutto, invece, non agisce su un'espressione, ma sugli argomenti di altri costrutti, per esempio:
\begin{lstlisting}
val f : Maybe a -> b -> String
let f
    | Nothing _ -> "Error_404"
    | (Just _) _ -> "Ok_200"
\end{lstlisting}
In qualsiasi caso, vi sono sempre uno o più valori da ispezionare e uno o più casi, i quali hanno uno o più \textit{case
expressions} (nel codice, spesso vengono nominate come \textit{matching expressions}) e un'espressione di ritorno.
Quindi abbiamo un costrutto della forma:
\[ match \; val_1 \; ... \; val_n \; with \; case_1 \; ... \; case_m \]
dove $ val_i $ è un token che definiremo come \textit{token top-level} o \textit{token di scrutinio} e
$ case_j $, che chiameremo \textit{caso}, ha la forma:
\[ me_{1j}, \; ... \; me_{nj} \rightarrow e_j \]
L'algoritmo di inferenza di tale costrutto è il seguente:
\begin{itemize}
    \item Inferisci $ val_i $, applica le sostituzioni all'ambiente di tipizzazione e poi fai lo stesso con $ val_{i+1} $,
    finché non termina la lista di token di scrutinio;
    \item nell'inferenza dei casi, dovrà valere:
        \[ \forall j. \; typeOf(val_i) \sqsubseteq typeOf(me_{ij}) \]
    ovvero le case expressions non potranno avere un tipo più generale dei corrispondenti token di scrutinio;
    \item nell'inferenza dei casi, è necessario tener traccia dei tipi dei casi già inferiti in precedenza, in
    quanto non sempre dai casi precedenti si inferisce il tipo più specializzato tra tutti i casi $ case_j $, quindi,
    i casi già inferiti verrano mantenuti;
    \item inferisci $ case_j $, applica le sostituzioni all'ambiente di tipizzazione e ai casi precedenti, poi passa
    a $ case_{j+1} $;
    \begin{itemize}
        \item nell'inferenza del caso j-esimo, il quale avrà la forma $ me_{1j} \; ... \; me_{nj} \rightarrow e_j $,
        verranno inferite, una alla volta, le case expressions $ me_{ij} $;
        \item inferisci come se fosse un argomento di una lambda astrazione $ me_{ij} $ la quale avrà mono-tipo $ \tau $,
        trova il tipo dell'i-esima case expression nei casi precedenti, sia esso $ \tau' $, poi effettua
        l'unificazione tra $ \tau $ e $ \tau' $ e applica le sostituzioni
        all'ambiente di tipizzazione e ai casi precedenti, poi continua con $ me_{i+1j} $ finché non termina la lista di
        case expressions;
        \item inferisci $ e_j $, la quale avrà mono-tipo $ \tau $, trova il tipo delle espressioni dei casi, sia esso
        $ \tau' $, effettua l'unificazione tra $ \tau $ e $ \tau' $ e applica le sostituzioni all'ambiente di tipizzazione
        e ai casi precedenti.
    \end{itemize}
\end{itemize}

\paragraph{Eliminazione del pattern matching "in larghezza"}
Come è stato già menzionato precedentemente, langgg espone due sintassi diverse per effettuare il pattern matching,
una delle quali può possedere un numero arbitrario di case-expressions. Tuttavia, il token che rappresenta un
\textit{caso} del costrutto di pattern matching può avere solamente una \textit{matching expression} (equivalente
a una case expression), la sua definizione si trova in \texttt{Compiler.Ast.Typed}:
\begin{lstlisting}
data NotedCase a = NotedCase (NotedMatchExpr a) (NotedExpr a) a
\end{lstlisting}
Questo rende necessaria una fase di desugaring che avviene subito dopo l'inferenza di un costrutto di pattern matching,
quindi durante la fase di type-inference. Si consideri il seguente codice langgg:
\begin{lstlisting}
type Maybe a = Nothing | Just a
type Either a b = Left a | Right b

let f
    | Nothing _ -> 42
    | (Just x) (Left y) -> x + y
    | _ (Right y) -> y
\end{lstlisting}
Dopo aver inferito il tipo dell'espressione legata ad \texttt{f}, vengono creati gli argomenti di \texttt{f}, i quali
sono legati alle case expressions nell'espressione di pattern matching, siano essi \texttt{v1} e \texttt{v2}. %TODO
\begin{lstlisting}
\end{lstlisting}

\subsubsection{Constraints}
La sintassi langgg permette di esprimere espressioni il cui tipo possiede dei predicati; ciò richiede che l'algoritmo
classico di type-inference venga esteso. L'algoritmo esteso di langgg si ispira a [2], in cui Martin Sulzmann,
Martin Odersky, Martin Wehr presentano un'estensione di HM che supporta anche tipi che possiedono predicati. I giudizi
di tipo vengono estesi con un'ipotesi aggiuntiva sui constraints e avranno la forma:
\[ CS, \Gamma \vdash_W e : \tau \]
Perciò l'ambiente di tipizzazione viene esteso con un altro "attore", ovvero un contesto di soddisfabilità. Nel caso
di langgg, il contesto di soddisfabilità è dato dall'insieme delle istanze.
L'articolo presenta inizialmente il problema di come ridefinire le regole istanziazione (\textbf{Inst}) e generalizzazione
(\textbf{Gen}). Sull'istanziazione di uno schema di tipo si arriva subito alla seguente conclusione:
\[ \infer[\textbf{Inst}]{CS, \Gamma \vdash_W e : \{ \overline{\tau} / \overline{\alpha} \}\tau'}{CS, \Gamma \vdash_W e : \forall \overline{\alpha}. D \Rightarrow \tau' & CS \vdash^e \{ \overline{\tau} / \overline{\alpha} \} D} \]
Questa regola sostiene che, quando si istanzia uno schema di tipo $ \forall \overline{\alpha}. D \Rightarrow \tau $, gli
unici mono-tipi validi sono quelli della forma $ \{ \overline{\tau} / \overline{\alpha} \} \tau' $ e tali che il constraint
$ \{ \overline{\tau} / \overline{\alpha} \}D $ \textit{soddisfa} uno o più constraints nel contesto di soddisfabilità $ CS $.
Di seguito è riportato in pseudo-codice l'algoritmo di soddisfabilità dei constraints, definito in
\texttt{Compiler.Ast.Typed}. Esso si basa sull'unificazione \textit{strict}:
\begin{lstlisting}
satisfy(C => args, C' => args'):
    C == C' and
    areEq(args, args')
\end{lstlisting}
dove \texttt{areEq} è l'algoritmo che effettua il test di specializzazione tra i vari tipi di \texttt{args}
e \texttt{args'}, eseguendo, successivamente, il test di uguaglianza tra mono-tipi:
\begin{lstlisting}
areEq(args, args'):
    match moreSpecOneByOne(args, args', []) with
        failure -> false
        args'' -> args == args1''

moreSpecOneByOne(args, args', resArgs):
    match args, args' with
        [] [] -> resArgs
        [] _ -> fail
        _ [] -> fail
        (ty : t) (ty' : t') ->
            let subst = isMoreSpec(ty, ty') in
            let t' = apply(subst, t') in
            let resArgs = resArgs ++ [apply(subst, ty')] in
                moreSpecOneByOne(t, t', resArgs)
\end{lstlisting}
la funzione \texttt{apply} prende in input una sostituzione e uno o più token che vengono ritornati dopo che è stata
applicata loro la sostituzione. La funzione \texttt{isMoreSpec} è invece il test di specializzazione (unificazione
\textit{strict}) tra due mono-tipi; infatti, come da definizione, gli argomenti dei constraints sono mono-tipi.
Si ricordi che il test di unificazione può fallire. In generale, diremo che un constraint $ C $ \textit{soddisfa} un
constraint $ C' $ se vale $ satisfy(C, C') $; viceversa, diremo che un constraint $ C $ \textit{è soddisfatto} da un
constraint $ C' $ se valie $ satisfy(C', C) $.
Si noti come, a causa dell'utilizzo dell'unificazione \textit{strict}, il test soddisfabilità tra constraints, non sia
commutativo. Si osservi il seguente esempio in langgg:
\begin{lstlisting}
property Foo a b c =
    <metodi>
;;
\end{lstlisting}
Supponiamo che esistano le seguenti istanze di \texttt{Foo}:
\begin{lstlisting}
Foo Char a b
Foo Int Int Int
Foo a String Int
\end{lstlisting}
Il constraint \texttt{Foo String String Int} è soddisfatto in quanto esiste l'istanza \texttt{Foo a String Int} che
soddisfa il constraint. La non-commutatività si nota subito, infatti, \texttt{Foo String String Int} non soddisfa
texttt{Foo a String Int}, in quanto la variabile di tipo \texttt{a} non è più specializzata (o meno generale) rispetto
al tipo concreto \texttt{String}. Il constraint \texttt{Foo x Char y}, invece, non è soddisfatto in quanto nessuna
delle tre istanze lo soddisfa.

Ora che è stata presentata formalmente la nozione di soddisfabilità, possiamo procedere con la ridefinizione della
regola di generalizzazione. Nell'articolo [2] viene proposta la seguente regola:
\[ \infer[\textbf{Gen}]{CS \wedge \exists \overline{\alpha}. D, \Gamma \vdash_W e : \forall \overline{\alpha}. D \Rightarrow \tau}{CS \wedge D, \Gamma \vdash_W e : \tau & \overline{\alpha} \notin free(\Gamma) \cup free(CS)} \]
Si noti come vi è un nuovo quantificatore: $ \exists \overline{\alpha}. D $. Questa notazione è utile per quantificare
le variabili di tipo all'interno di un constraint.

Per quanto riguarda l'algoritmo utilizzato in langgg, vi è un ulteriore contesto/ambiente di cui è necessario tener conto,
ovvero quello dei problemi di constraints correnti. Questa ulteriore estensione dell'ambiente di tipizzazione è utile
per mantenere i constraints che sono presenti nell'attuale contesto di inferenza. In seguito, vedremo in che modo
viene utilizzato. Lo indicheremo con $ \Omega $.

Indipendentemente dalle regole \textbf{Inst} e \textbf{Gen} ridefinite, è necessario fissare un comportamento che
riguarda le altre regole di inferenza. In particolare, è necessario decidere il momento in cui viene eseguito l'algoritmo
di soddisfabilità dei constraints presenti in $ \Omega $. Questa azione è detta anche \textit{normalizzazione}.
In [2], vi è la proposta di nuove regole di inferenza in cui la normalizzazione viene effettuata alla fine delle
premesse di ogni regola di inferenza, eccetto quella della lambda-astrazione. Tuttavia, sempre nell'articolo, vi è
un'ulteriore proposta nella quale la normalizzazione viene effettuata in maniera \textit{lazy}, ovvero appena dopo
la generalizzazione ed è questa la politica utilizzata da langgg. Rimangono alcune questione aperte:
\begin{enumerate}
    \item quand'è che un predicato viene aggiunto a $ \Omega $?
    \item quali sono i constraints (se esistono) da normalizzare dopo che è stata effettuata la generalizzazione?
\end{enumerate}
Per rispondere a entrambe le domande, mostriamo come avviene l'inferenza di tipo in langgg in presenza di constraints.
Osserviamo il seguente esempio di codice langgg:
\begin{lstlisting}
property Foo a b c =
    val foo : a -> b -> c -> Char
;;

let f x y z = foo x y z
\end{lstlisting}
Andando a inferire l'espressione legata ad \texttt{f}, un token che viene incontrato è \texttt{foo}. Il suo schema di
tipo associato è $ \forall a, b, c. \; Foo \; a \; b \; c \Rightarrow a \mapsto b \mapsto c \mapsto Char $. A questo
punto, si può applicare la regola \textbf{Var}, che, a sua volta, applica la regola
\textbf{Inst}. Si ricordi che la politica di langgg sulla normalizzazione è \textit{lazy}, perciò
il constraint $ Foo \; a \; b \; c $ non viene normalizzato immediatamente, bensì viene aggiunto all'ambiente dei
problemi di constraints. $ \Omega $ si rende quindi necessario proprio a questo fine, ovvero ritardare la normalizzazione.
\`E bene notare che esso non compare nelle regole di inferenza; questo è possibile in quanto la politica \textit{lazy}
può essere considerata come \textit{side-effect} che non cambia la semantica della valutazione dei constraints.
Quindi la conclusione della regola \textbf{Var} in questo caso sarà semplicemente:
\[ CS, \Gamma \vdash_W foo : a1 \mapsto a2 \mapsto a3 \mapsto Char \]
dove $ a1 $, $ a2 $ e $ a3 $ sono variabili nuove. Continuando ad applicare le regole di inferenza \textbf{App} e
\textbf{Var} si arriverà ad avere, prima di applicare \textbf{Let} per generalizzare \texttt{f}, il seguente
ambiente di tipizzazione:
\[ \Gamma = \{ x : a1, y : a2, z : a3 \} \]
\[ \Omega = \{ Foo \; a1 \; a2 \; a3 \} \]
\[ CS = \emptyset \]
Applicando la regola \textbf{Let}, si applica anche la regola \textbf{Gen}; in questo caso, l'unico constraint
nel contesto è $ Foo \; a1 \; a2 \; a3 $ e il mono-tipo da generalizzare associato a \texttt{f} è
$ \tau = a1 \mapsto a2 \mapsto a3 \mapsto Char $. Rimane da decidere quando un constraint deve essere aggiunto allo
schema e quando, eventualmente, deve essere normalizzato. Innanzitutto, è necessario osservare le variabili libere
del mono-tipo $ \tau $, siano esse $ \overline{\alpha} = free(\tau) $. Un constraint $ C $ viene aggiunto allo schema
di tipo se:
\[ free(C) \subseteq \overline{\alpha} \wedge free(C) \neq \emptyset \]
La prima condizione richiede che tutte le variabili libere nel constraint siano presenti anche nel mono-tipo, mentre
la seconda condizione previene l'occorrenza di constraints senza variabili di tipo. Nel caso in cui la condizione non
venga rispettata, il constraint $ C $ deve essere normalizzato. Tornando all'esempio precedente, si ha che
il constraint $ Foo \; a1 \; a2 \; a3 $ può essere aggiunto allo schema di tipo che verrà generato.
\texttt{f} avrà, quindi, il seguente tipo:
\[ \forall a1, a2, a3. \; Foo \; a1 \; a2 \; a3 \Rightarrow a1 \mapsto a2 \mapsto a3 \mapsto Char \]
Si osservi ora quest'altro esempio:
\begin{lstlisting}
property Bar a b =
    val bar : a -> b -> Char
;;

let z = z

let g x y = (foo x y z, bar x y)
\end{lstlisting}
Nell'applicazione delle regole di inferenza, si ottiene che l'espressione legata a \texttt{g} ha tipo
\texttt{(Char, Char)} e prima di applicare la regola \textbf{Let}, è naturale pensare all'ambiente dei problemi di
constraints siffatto:
\[ \Omega = \{ Foo \; a1 \; a2 \; b, Bar \; a1 \; a2 \} \]
e al mono-tipo da generalizzare:
\[ \tau = a1 \mapsto a2 \mapsto Char \]
Al momento della generalizzazione, si ha che:
\[ free(Foo \; a1 \; a2 \; b) \not\subseteq free(\tau) \wedge free(Bar \; a1 \; a2) \subseteq free(\tau) \]
Di conseguenza, il constraint $ Bar \; a1 \; a2 $ può essere aggiunto allo schema di tipo, mentre
$ Foo \; a1 \; a2 \; b $ deve essere normalizzato in quanto la variabile di tipo $ b $ non compare nel mono-tipo $ \tau $.

\paragraph{Normalizzazione}
Per quanto riguarda la normalizzazione, è stato già menzionato che essa corrisponde alla seguente operazione:
\begin{lstlisting}
normalize(C, instances):
    isSatisfiable(C, instances)
\end{lstlisting}
Tuttavia, è necessario perfezionare l'implementazione, in quanto, al momento, non è possibile selezionare l'istanza
che soddisfa il constraint (cfr. dispatch statico). L'istanza che soddisfa il constraint deve essere unica,
in quanto, è necessario non avere ambiguità sull'implementazione da scegliere. Chiaramente, se non vi sono istanze
che soddisfano un constraint, il programma viene respinto con un errore. Bisogna, quindi, fissare una semantica
di inferenza di istanza nel caso in cui vi siano due o più istanze che soddisfano un constraint. La scelta di langgg è
quella di selezionare l'istanza "più specializzata". Definiamo ora una relazione d'ordine parziale tra constraints,
siano $ C $ e $ C' $ due constraints:
\begin{itemize}
    \item se $ satisfy(C, C') \wedge satisfy(C', C) $, allora $ C = C' $;
    \item se $ satisfy(C, C') \wedge \neg(satisfy(C', C)) $, allora $ C > C' $;
    \item se $ \neg(satisfy(C, C')) \wedge satisfy(C', C) $, allora $ C < C' $;
    \item se $ \neg(satisfy(C, C')) \wedge \neg(satisfy(C', C)) $, allora la relazione d'ordine è indefinita.
\end{itemize}
dove l'operatore $ \neg $ è la naturale negazione booleana. Di seguito, vi è definito l'algoritmo di selezione dell'istanza
ispirato da [13]:
\begin{enumerate}
    \item l'input è una sequenza $ cs $ di constraints;
    \item scarta da $ cs $ tutti quei constraints $ c $ tali che esiste $ c' \in cs $ che non sia $ c $ e tale che
    $ c' > c $;
    \item se alla fine vi rimane più di un constraint, allora non è possibile selezionare un'istanza; se, invece, vi
    è una sola istanza rimasta, allora essa viene selezionata.
\end{enumerate}
Il fatto che la relazione d'ordine sia parziale non rappresenta un problema, infatti, le istanze vengono scartate solo
se la relazione d'ordine è definita.

\paragraph{Confronto con Haskell}
langgg ha un sistema di tipi molto simile a quello di Haskell, il quale, a sua volta, possiede un constraint-system
"costruito" sopra HM. Anche Haskell ha una politica \textit{lazy} di risoluzione delle istanze, ma, a differenza di langgg,
in caso di molteplici istanze soddisfatte, l'azione di default è quella di rifiutare il programma, a meno che l'utente
non utilizzi alcune estensioni del linguaggio (cfr. [13]). La politica di langgg è più elastica e permette all'utente
di avere più istanze valide per certi constraints, tuttavia, ciò rappresenta una responsabilità in più da parte
dell'utente, il quale deve essere consapevole di come avviene l'inferenza di istanza. Vi è quindi un tradeoff tra
elasticità e consapevolezza dell'utente.

\subsubsection{Type-hinting}
langgg permette all'utente di indicare il tipo di un'espressione (o di un simbolo). Questo comporta una modifica
all'algoritmo di inferenza di tipo. Data un'espressione con type-hinting, lo schema generale è inferire prima
l'espressione e successivamente "applicare" il type-hinting. Per quanto riguarda i casi ricorsivi delle espressioni
(tutti i casi fuorché i simboli, i letterali e i data-constructors), il tipo proveniente dal type-hinting viene
"scomposto" in più sotto-tipi che vengono utilizzati come type-hinting nell'inferenza dei casi ricorsivi. Tale
scomposizione è definita sui tipi funzione ed è chiamata, all'interno del codice sorgente, operazione di
\textit{unfold}:
\begin{lstlisting}
unfoldType(ty):
    match ty with
        (ty1 -> ty2) -> ty1 : unfoldType ty2
        _ -> [ty]
\end{lstlisting}
Questa operazione è molto utile, ad esempio, nell'inferenza della lambda astrazione. Si osservi il seguente sorgente
langgg:
\begin{lstlisting}
(lam x -> f x) : Char -> String
\end{lstlisting}
Il tipo \texttt{Char -> String} viene scomposto in \texttt{Char} e \texttt{String}. Nell'inferenza dell'argomento
\texttt{x}, il type-hinting sarà esattamente \texttt{Char}, mentre per l'inferenza dell'espressione \texttt{f x}
il type-hinting sarà \texttt{String}.

Per quanto riguarda, invece, i casi di base, lo schema è:
\begin{itemize}
    \item cerca nell'ambiente di tipizzazione il tipo del token, sia esso lo schema di tipo $ \sigma $;
    \item applica la regola di istanziazione su $ \sigma $, sia $ \tau $ il mono-tipo risultante;
    \item sia $ \pi $ il mono-tipo istanziato dallo schema di tipo proveniente dal type-hinting;
    \item effettua l'unificazione \textit{strict} tra $ \pi $ e $ \tau $; si ricordi che tale operazione, a differenza
    dell'unificazione originale, non gode della proprietà commutativa, in quanto l'azione di \textit{swap} non è
    ammessa. Il mono-tipo $ \pi $ non può essere modificato dopo la sostituzione;
    \item applica l'unificazione all'ambiente di tipizzazione e al token tipato.
\end{itemize}
La regola è molto simile a quella senza type-hinting, tuttavia, viene effettuata un'unificazione in più.
Dato che l'insieme dei letterali e dei data-constructors all'interno del linguaggio non può avere variabili libere,
un'ottimizzazione che viene applicata ai loro casi è non applicare la sostituzione
all'ambiente di tipizzazione. Un'operazione che, in generale, è piuttosto dispendiosa.

\subsubsection{Ricorsione}
Il lambda-calcolo polimorfico (\textit{System-F}) non ammette la ricorsione, inoltre, non è nemmeno possibile
trovare il tipo per il cosiddetto \textit{combinatore Y}, un modo per definire la ricorsione in linguaggi che
generalmente non la supportano. Questo rende \textit{System-F} non Turing-completo, in quanto qualsiasi programma
termina. langgg (come anche Haskell e molti altri linguaggi funzionali) ammette, invece, una sintassi per la ricorsione.
Questo fatto porta con sé alcune conseguenze molto importanti. Innanzitutto, la premessa della regola \textbf{Var}
non è vera per le funzioni ricorsive, in quanto con la presenza della ricorsione i simboli possono apparire in un
programma prima ancora che il loro tipo venga generalizzato. Inoltre, sarebbe impossibile ordinare i bindings in base
alle loro dipendenze. Per questo motivo, i bindings ricorsivi vengono raggruppati
(cfr. Clusters di definizioni mutualmente ricorsive) e considerati come un unico binding della forma:
\[ rec \; v_1 = e_1 \; and \; v_2 = e_2 \; and \; ... \; v_n = e_n \]
Tuttavia, il problema della regola \textbf{Var} non è ancora risolto, quindi è necessario definire una nuova regola
che riguarda i bindings ricorsivi:
\[ \infer[\textbf{Rec}]{\Gamma \vdash_W rec \; v_1 = e_1 \; and \; v_n = e_n \; in \; e : \tau }{\Gamma, \Gamma' \vdash_W e_1 : \tau_1 & ... & \Gamma, \Gamma' \vdash_W e_n : \tau_n & \Gamma, \Gamma'' \vdash_W e : \tau } \]
dove:
\[ \Gamma' = v_1 : \tau_1, ..., v_n : \tau_n \]
\[ \Gamma'' = v_1 : \overline{\Gamma}(\tau_1), ..., v_n : \overline{\Gamma}(\tau_n) \]
La regola inferisce prima tutte le espressioni $ e_i $ e poi applica la regola di generalizzazione \textbf{Gen} soltanto
alla fine. Nell'algoritmo di inferenza di langgg, prima di inferire un cluster di bindings ricorsivi, vengono creati
nuovi mono-tipi della forma di variabili di tipo, i quali vengono assegnati ai bindings del cluster. Durante l'inferenza,
questi mono-tipi possono essere specializzati. In questo modo, si può applicare la regola
\textbf{Var} ad ogni occorrenza di simbolo ricorsivo. L'istanziazione di un mono-tipo non rappresenta un problema
in quanto le variabili libere non vengono istanziate. In questo caso, l'algoritmo di istanziazione corrisponde
alla funzione identità.

\section{Generazione del codice}
L'output dell'algoritmo di type inference è un \texttt{TypedProgram}, una tabella che mantiene i bindings tipati.
La generazione del codice consiste in tre sotto-fasi:
\begin{enumerate}
    \item alcuni task che riguardano il desugaring, necessari per la generazione del codice;
    \item traduzione dai token tipati di \texttt{Compiler.Ast.Typed} ai token della sintassi del linguaggio Core;
    \item esecuzione del back-end di Haskell, il quale genera il codice a basso livello di target differenti.
\end{enumerate}
Di seguito, lo schema delle fasi finali del compilatore:
\newline

\begin{tikzpicture}[node distance=2cm]
\node (typedProg) [object] {Bindings tipati};
\node (staticDispatch) [object, below of=typedProg] {Bindings tipati};
\draw [arrow] (typedProg) -- node[anchor=east] {Dispatch statico} (staticDispatch);
\node (lambdas) [object, below of=staticDispatch] {Bindings tipati};
\draw [arrow] (staticDispatch) -- node[anchor=east] {Lambda-astrazione} (lambdas);
\node (deepPm) [object, below of=lambdas] {Bindings tipati};
\draw [arrow] (lambdas) -- node[anchor=east] {Rimozione del pattern matching "profondo"} (deepPm);
\node (scrutinee) [object, below of=deepPm] {Bindings tipati};
\draw [arrow] (deepPm) -- node[anchor=east] {Unificazione degli scrutini} (scrutinee);
\node (coreGen) [object, below of=scrutinee] {Sintassi Core};
\draw [arrow] (scrutinee) -- node[anchor=east] {Generazione della sintassi Core} (coreGen);
\node (backend) [object, below of=coreGen] {Codice finale};
\draw [arrow] (coreGen) -- node[anchor=east] {Back-end Haskell} (backend);
\node (desugar) [object, right of=lambdas, xshift=4cm] {Desugaring};
\draw [arrow] (desugar) -- (staticDispatch);
\draw [arrow] (desugar) -- (lambdas);
\draw [arrow] (desugar) -- (deepPm);
\draw [arrow] (desugar) -- (scrutinee);
\end{tikzpicture}

\subsection{Fasi di desugaring}

\subsubsection{Dispatch statico}
Il polimorfismo ad-hoc viene implementato attraverso il meccanismo delle type-classes (cfr. Polimorfismo ad-hoc).
La sintassi Core non possiede, però, costrutti per i metodi delle type-classes e per i bindings delle istanze.
La politica del compilatore langgg è quella di aggiungere i bindings delle istanze ai bindings del programma,
cambiando i nomi degli stessi metodi di istanze diverse (cfr. Costruzione delle istanze). Questa politica ha
importanti implicazioni: innanzitutto, nelle espressioni del programma, i metodi delle istanze possono essere chiamati,
ma essi possono essere nominati solamente tramite gli identificatori dei metodi delle proprietà. Ad esempio:
\begin{lstlisting}
property Foo a =
    val foo : a -> a
;;

instance Foo String =
    let foo s = s
;;

let f x = (x, foo "42")
\end{lstlisting}
Come è stato già esposto in precedenza, il metodo \texttt{foo} dell'istanza \texttt{Foo Int} avrà un identificatore
diverso da \texttt{foo}. Tuttavia, \texttt{foo} viene nominato nell'espressione legata a \texttt{f}. \`E necessario,
quindi, un modo per trasformare, eventualmente, le occorrenze dei metodi di proprietà con i giusti metodi di istanza.
Si pensi a quest'altro pezzo di codice langgg:
\begin{lstlisting}
type Box a = Box a

val g : Foo (Box a) => a -> Box a
let g x = foo (Box x)

let h x = g x
\end{lstlisting}
\`E chiaro come la chiamata di \texttt{foo} all'interno dell'espressione legata a \texttt{g} dipenda da quale istanza
della forma \texttt{Foo (Box a)} venga utilizzata. Vale lo stesso per \texttt{g} nei confronti dell'espressione legata
ad \texttt{h}. In linea generale, il suddetto problema è legato al polimorfismo ad-hoc e viene risolto attraverso un
metodo di dispatch statico. Il polimorfismo ad-hoc permette di creare funzioni che hanno comportamenti diversi a
seconda del loro tipo. Il dispatch statico si occupa di "scegliere" correttamente le funzioni a disposizione nel
programma (i metodi d'istanza) che implementano i relativi metodi di proprietà nel programma. A differenza di molti
linguaggi object-oriented, in cui la selezione dei metodi avviene a run-time attraverso meccanismi di dispatch dinamici,
in langgg la selezione viene risolta completamente a compile-time, evitando così l'overhead che può essere causato
da un algortimo di dispatch dinamico. In particolare, il dispatch statico in langgg viene implementato attraverso
la \textit{monomorfizzazione} [11], già introdotta, in realtà, nella costruzione delle istanze. Ora renderemo la
monomorfizzazione generale a tutte le funzioni del programma; l'implementazione che seguirà si ispira a un articolo
sul blog di Jeremy Mikkola [12]. Si ripensi alla precedente funzione \texttt{g}. Il suo
schema di tipo ha con sè dei predicati. In questo caso, durante la fase di type-inference, alla funzione \texttt{g}
verrà aggiunto un parametro che corrisponderà al predicato \texttt{Foo (Box a)}, ovvero stiamo trattando il tipo di
\texttt{g} come se fosse:
\begin{lstlisting}
val g : Foo (Box a) -> a -> Box a
\end{lstlisting}
\`E necessario estendere la definizione di tipo qualificato:
\[ QT := MT \; | \; C \; | \; C \Rightarrow QT \]
Quindi, un tipo qualificato in langgg può possedere soltanto constraints. Questa eventualità viene tuttavia limitata
a pochi casi possibili e, comunque, langgg non espone una sintassi per esprimere tipi qualificati che possiedono
soltanto constraints.

Sempre durante la fase di type-inference, ogni volta che la funzione \texttt{g} viene utilizzata nel programma, oltre
alla normale applicazione della regola di inferenza \textbf{Var} e alla generazione del token tipato che rappresenta
una variabile, verrà generata e applicata a \texttt{g} un'ulteriore variabile. Quest'ultima viene associata al
relativo problema di constraint e viene salvata nell'ambiente di tipizzazione. Al momento della generalizzazione,
dai problemi di constraint non normalizzati vengono estratte le variabili associate, le quali diventano parte degli
argomenti del binding. Ad esempio, l'implementazione della funzione \texttt{h} a livello di token tipati sarà:
\begin{lstlisting}
let h v1 x = g v1 x
\end{lstlisting}
dove \texttt{v1} è la variabile precedentemente menzionata, detta anche \textit{variabile di dispatch}. Le espressioni
che contengono le variabili di dispatch sono definite in \texttt{Compiler.Ast.Typed}:
\begin{lstlisting}
data DispatchTok a =
      DispatchVar (NotedVar a) a
    | DispatchVal (OnlyConstraintScheme a) a
\end{lstlisting}
Il primo caso (\texttt{DispatchVar}) rappresenta una variabile di dispatch. Il secondo caso, invece, rappresenta un
\textit{valore di dispatch}, il quale è costituito da uno schema di tipo contenente solamente constraints ed è utile
per "selezionare" l'implementazione giusta di una funzione.

La fase di type-inference, oltre a un programma tipato, ritorna anche una sequenza di nomi di simboli da ridefinire.
Questi ultimi sono tutti quei simboli che compaiono nel programma nella cui espressione vi sono valori di constraints.
Se in un'espressione un simbolo ha solo valori di dispatch applicati (e non variabili di dispatch), allora è necessario
generare un nuovo simbolo il cui identificatore univoco viene creato a partire dai valori di dispatch, come è avvenuto
per i metodi di istanza.
Nel modulo \texttt{Compiler.Desugar.AdHoc} viene implementato un algoritmo di monomorfizzazione che, ricorsivamente,
cerca nelle espressioni tutte le occorrenze di simboli a cui sono applicati valori di constraints e
genera le conseguenti implementazioni.
Ovviamente, la generazione di nuove implementazioni può dar vita a nuove espressioni con valori di constraints,
quindi l'algoritmo agisce ricorsivamente finché non esistono più implementazioni da generare:
\begin{enumerate}
    \item l'input è una serie di bindings $ bs $ in cui cercare i valori di dispatch;
    \item $ \forall b \in bs $, vengono cercati, nell'espressione legata a $ b $, tutte le occorrenze di simboli che
    hanno applicati solo valori di dispatch;
    \item ogni occorrenza viene sostituita con un nuovo identificatore univoco in tutto il programma costituito da:
        \[<symid><dispatchval_1>...<dispatchval_n>\]
    dove $ <symid> $ è l'identificatore originale e $ <dispatchval_i> $ è l'i-esimo valore di dispatch applicato;
    \item $ \forall sym $ che viene sostituito, $ sym $ viene aggiunto alla lista $ rs $ di simboli da creare, con le
    informazioni sui valori di dispatch che servono per creare il nuovo identificatore;
    \item $ \forall r \in rs $, viene creato un nuovo binding in cui:
    \begin{itemize}
        \item l'identificatore è costruito utilizzando il metodo menzionato precedentemente;
        \item gli argomenti di dispatch spariscono;
        \item nell'espressione associata, vengono cercate tutte le occorrenze delle variabili di dispatch, le quali
        vengono sostituite dai valori di dispatch associati a $ r $;
        \item applica i passi (3) e (4) per ogni occorrenza di simbolo che ha soltanto valori di dispatch applicati;
    \end{itemize}
    \item ripeti (5) finché non terminano i simboli in $ rs $.
\end{enumerate}

\subsubsection{Lambda-astrazione}
Riportiamo la definizione di binding:
\begin{lstlisting}
type BindingSingleton a = (NotedVar a, [NotedVar a], NotedExpr a)
\end{lstlisting}
Come si può notare dalla definizione, un binding è una tripla costituita da un identificatore, degli argomenti e
un'espressione. In questa fase, vengono modificate tutte le espressioni in modo da eliminare gli argomenti dalla
tripla. Di preciso, gli argomenti del binding diventano argomenti di un costrutto di lambda-astrazione che ha come
espressione l'espressione legata del binding. Si ricordi che, durante la type inference, gli argomenti di un binding
vengono trattati come argomenti di un costrutto di lambda-astrazione (cfr. Implementazione delle regole di inferenza).

\subsubsection{Rimozione del pattern matching profondo}
Si consideri il seguente sorgente langgg:
\begin{lstlisting}
type Success = Ok200
type Error = Undefined | Err401 | Err404 | Err500
type ErrType = ServerError | ClientError

type Maybe a = Nothing | Just a
type Either a b = Left a | Right b

let f x =
    match x with
          Nothing -> Just ServerError
        | Just (Right Ok200) -> Nothing
        | Just (Left Err401) -> Just ClientError
        | Just (Left Err404) -> Just ClientError
        | Just _ -> Just ServerError
\end{lstlisting}
Il codice relativo alla funzione \texttt{f} è semanticamente equivalente a:
\begin{lstlisting}
let f x =
    match x with
          Nothing -> Just ServerError
        | Just x1 ->
            (match x1 with
                  Right x2 ->
                    (match x2 with
                        Ok200 -> Nothing
                    )
                | Left x2 ->
                    (match x2 with
                          Err401 -> Just ClientError
                        | Err404 -> Just ClientError
                        | _ -> Just ServerError
                    )
            )
\end{lstlisting}
La differenza tra le due versioni è che nella seconda i data-constructors appaiono solo come teste (e non come argomenti)
nelle \textit{case expressions}. In seguito, vedremo perché è importante che valga questa proprietà (cfr. Generazione del
codice Core). Questa fase di desugaring trasforma le espressioni di pattern matching, modificando le
\textit{case expressions}, in modo che ad ogni loro argomento che contiene un data-contructor, viene creata una nuova
expressione di pattern matching innestata. Il codice relativo a questa fase si trova in \texttt{Compiler.Desugar.DeepPM}.

\subsubsection{Unificazione degli scrutini}
Questa è un'altra fase di desugaring che riguarda le espressioni di pattern matching. Le motivazioni di questa
fase verranno chiarite in seguito (cfr. Generazione del codice Core). Prima di entrare nei dettagli di questa fase,
è necessario definire cosa sono le \textit{variabili di scrutinio} o più semplicemente \textit{scrutini}. Sia
$ match \; e \; with \; m_1 \rightarrow e_1 \; ... \; m_n \rightarrow e_n $ un'espressione di pattern matching,
chiameremo \textit{scrutinio} la variabile legata direttamente al valore dell'espressione $ e $. Ad esempio:
\begin{lstlisting}
let f x y =
    match g x y with
          Just v -> Just (v + y)
        | a -> Just y
        | b -> b
\end{lstlisting}
Nell'espressione legata ad \texttt{f}, le variabili di scrutinio sono \texttt{a} e \texttt{b}, mentre la variabile
\texttt{v} non lo è. Ciò che avviene nella fase di desugaring è la trasformazione delle case-expressions in modo che
una sola variabile di scrutinio venga utilizzata, quindi il codice langgg precedentemente mostrato diventerà:
\begin{lstlisting}
let f x y =
    match g x y with
          Just v -> Just (v + y)
        | v1 -> Just y
        | v1 -> v1
\end{lstlisting}
Si noti che le variabili delle sotto-espressioni vengono opportunamente aggiornate.

\subsection{Generazione del codice Core}
Core è una rappresentazione intermedia utilizzata da GHC come rappresentazione intermedia di Haskell. La generazione
del codice Core è la fase finale del front-end del compilatore langgg. Core è un linguaggio minimale ed è una variante
esplicitamente tipata (a differenza di Haskell e langgg) del formalismo \textit{System-F} [3]. L'obiettivo del compilatore
langgg è costruire dei token tipati e manipolarli in modo che la traduzione dal programma tipato langgg all'equivalente
Core sia più lineare possibile. GHC espone al cliente delle API che permettono di utilizzare le funzionalità del
compilatore [14], comprese quelle per creare e manipolare un programma Core. Di seguito vi è il tipo di dato algebrico
utilizzato nelle API di GHC per rappresentare un'espressione Core:
\begin{lstlisting}
data Expr b
  = Var   Id
  | Lit   Literal
  | App   (Expr b) (Arg b)
  | Lam   b (Expr b)
  | Let   (Bind b) (Expr b)
  | Case  (Expr b) b Type [Alt b]
  | Cast  (Expr b) Coercion
  | Tick  (Tickish Id) (Expr b)
  | Type  Type
  | Coercion Coercion

type Arg b = Expr b

type Alt b = (AltCon, [b], Expr b)

data AltCon
  = DataAlt DataCon
  | LitAlt  Literal
  | DEFAULT

data Bind b = NonRec b (Expr b)
            | Rec [(b, (Expr b))]
\end{lstlisting}
I commenti sono stati rimossi, cfr. [15] per il codice sorgente originale. Il tipo \texttt{Expr} rappresenta
un'espressione Core, mentre l'alias di tipo \texttt{Alt} rappresenta un \textit{caso} del costrutto di pattern matching.
Il tipo \texttt{Bind}, invece, rappresenta i bindings di Core.
L'obiettivo finale è costruire un token \texttt{CoreProgram}:
\begin{lstlisting}
type CoreProgram = [CoreBind]

type CoreBind = Bind CoreBndr

type CoreBndr = Var
\end{lstlisting}
Il tipo \texttt{Var} è rappresenta le variabili in Core con un informazione sul loro tipo. Nel contesto di Core, con il
\textit{variabile} si intende un concetto di ampio spettro, infatti, \texttt{Var} comprende le variabili di un programma,
le variabili di tipo, le variabili di kind, etc.. Come si può notare dalle strutture dati utilizzate nelle API di GHC e
dalle fasi precedenti del compilatore Core, l'obiettivo del compilatore langgg è creare token tipati che possano essere
successivamente trasformati in token Core nel modo più lineare possibile. Alcuni esempi sono:
\begin{itemize}
    \item la definizione di binding di Core è molto simile a quella utilizzata dal compilatore langgg (cfr.
    Approccio a tabelle);
    \item i costrutti di pattern matching, dopo le varie fasi di desugaring, non possiedono costruttori innestati nelle
    case expressions, come si può notare dal costruttore di un \textit{caso} in Core:
    \begin{lstlisting}
type Alt b = (AltCon, [b], Expr b)
    \end{lstlisting}
    dove la variabile di tipo \texttt{b} rappresenterà una variabile Core (\texttt{CoreBndr});
    \item il costrutto di pattern matching di Core richiede come secondo parametro una variabile di scrutinio:
    \begin{lstlisting}
| Case  (Expr b) b Type [Alt b]
    \end{lstlisting}
    Il fatto che gli scrutini siano stati tutti unificati rappresenta un vantaggio in termini di implementazione;
    \item i bindings non hanno la nozione di argomenti, infatti, in langgg gli argomenti dei bindings sono stati
    trasformati in argomenti dei costrutti di lambda-astrazione (cfr. Lambda-astrazione);
    \item Le definizioni dei token tipati in langgg portano con sè l'informazione sul proprio tipo in modo che i token
    Core possano essere costruiti. Si ricordi che Core è un linguaggio esplicitamente tipato, quindi ogni costrutto che
    ha la nozione di tipo, ha il tipo esplicitato nella sintassi.
\end{itemize}
I dettagli implementativi dell'effettiva traduzione si trovano nel modulo \texttt{Compiler.Codegen.ToCore}.

\paragraph{Traduzione dei tipi e dei costruttori}
Il modulo \texttt{Compiler.Codegen.Type} si occupa di trasformare i token di langgg che rappresentano i tipi nei
corrispettivi token di Core [16]. Il tipo di dato algebrico nelle API di GHC per rappresentare un tipo in Core è
\texttt{Type}. A differenza di langgg in cui per ogni nozione diversa di tipo è stato creato un token diverso,
\texttt{Type} racchiude molte nozioni di tipo, tra cui: mono-tipo, schema di tipo, variabili di tipo, etc.. Dal
punto di vista delle API di GHC, \texttt{Type} è un tipo di dato astratto, infatti, sempre il modulo \texttt{Type}
espone una serie di funzioni per costruire i tipi in Core. Altri tipi di dati fondamentali in Core sono \texttt{TyCon} e
\texttt{DataCon}, definiti rispettivamente in [17] e [18], i quali rappresentano rispettivamente i type-constructors e
i data-constructors. In precedenza, quando è stato presentato il tipo di approccio (a tabelle) che utilizza il
compilatore langgg
per costruire e salvare i token tipati, è stato fatto un confronto con GHC (cfr. Confronto con GHC) che utilizza un
approccio molto diverso. Infatti, in GHC i token vengono salvati in una struttura a grafo. Questo metodo si
ripercuote soprattutto a livello programmatico, infatti, prendendo in considerazione \texttt{TyCon} e \texttt{DataCon},
vi è una mutua dipendenza tra i due token. Questo implica che per costruire un costruttore di tipo è
necessario avere a disposizione i data-constructors associati, mentre per costruire un data-constructor è necessario avere a
disposizione il costruttore di tipo associato. La conseguenza è la scrittura di codice mutualmente ricorsivo, tuttavia,
si ricordi che Haskell è un linguaggio con un modello di valutazione delle espressioni \textit{lazy} e questo aiuta a
prevenire la ricorsione infinita in codice mutualmente ricorsivo.

\subsection{Back-end}
La fase finale di compilazione di un programma langgg consiste nel compilare un programma Core in target differenti.
La pagina di documentazione ufficiale di GHC espone nei dettagli il ciclo di compilazione di un programma Haskell (e
di conseguenza di un programma Core) [19]. Lo schema semplificato è il seguente:
\newline

\begin{tikzpicture}[node distance=2cm]
\node (Core) [process] {Programma Core};
\node (Stg) [process, below of=Core] {Programma STG};
\draw [arrow] (Core) -- node[anchor=west] {pipeline di Core} (Stg);
\node (Cmm) [process, below of=Stg] {Programma C - -};
\draw [arrow] (Stg) -- node[anchor=west] {pipeline di STG} (Cmm);
\node (Asm) [process, below of=Cmm] {Codice assembly};
\node (C) [process, left of=Asm, xshift=-3cm] {Codice C};
\node (Llvm) [process, right of=Asm, xshift=3cm] {Codice Llvm};
\draw [arrow] (Cmm) -- (C);
\draw [arrow] (Cmm) -- (Asm);
\draw [arrow] (Cmm) -- (Llvm);
\end{tikzpicture}
\newline

STG è un'ulteriore rappresentazione intermedia di Haskell, ma, a differenza di Core, non è un linguaggio tipato.
Uno degli obiettivi più importanti di STG è rendere efficiente la valutazione delle espressioni sull'hardware standard.
C - - è un linguaggio C-like, sviluppato con l'obiettivo di essere generato dai compilatori di linguaggi ad alto livello.
Nella traduzione da un formalismo a un altro, vi sono vari passaggi (pipeline) in cui vengono effettuate, oltre alla
traduzione effettiva, alcune ottimizzazioni.

I tre target principali sono:
\begin{itemize}
    \item Codice C, che viene restituito in output come \textit{pretty-printing} dal codice C - -;
    \item Codice Assembly specifico di una macchina;
    \item Codice Llvm, portabile e ottimizzato per molte architetture.
\end{itemize}

\section{Sviluppi futuri}
langgg è un linguaggio sperimentale in evoluzione. Vi sono alcune \textit{features} che arricchiscono il linguaggio che
potrebbero essere implementate in futuro. Di seguito, sono elencate alcune caratteristiche ritenute interessanti da
aggiungere al linguaggio. In particolare, nella presentazione delle funzionalità aggiuntive, verranno analizzate prima le
basi teoretiche, discutendo i vantaggi che vengono portati al linguaggio (e, eventualmente, svantaggi) e i problemi
che vengono risolti grazie alle suddette funzionalità aggiuntive.

\subsection{Tipi lineari}
Un sistema di tipi \textit{lineari} è una caratteristica dei linguaggi di programmazione (specialmente funzionali) molto
interessante, la quale porta numerosi vantaggi all'utente in termini pratici, tuttavia, non è una \textit{feature}
largamente adottata nei linguaggi mainstream. Vi è stata una proposta di implementazione in GHC [20] che ha
successivamente dato vita a un'estensione di Haskell. Questa sezione si ispirerà alla proposta che è stata fatta per GHC:
verranno date alcune definizioni al fine di introdurre la nozione di \textit{tipo lineare} e verrà, a sua volta, proposta una
bozza di implementazione nel compilatore langgg. Due vantaggi pratici che offre un sistema di tipi lineari sono: l'utilizzo
e il controllo della mutabilità con interfacce \textit{pure}; maggiore controllo in computazioni che riguardano l'IO.
In generale, i tipi lineari offrono maggiori garanzie all'utente programmatore.

Innanzitutto, prima di dare qualsiasi definizione, è bene precisare
come nell'articolo [20] i tipi non vengono divisi in due macro-insiemi (lineari e non-lineari), bensì la linearità viene
associata al tipo funzione. Informalmente, una funzione è \textit{lineare} se consuma il suo argomento una e una sola
volta. Indicheremo con $ \triangleright $ il tipo funzione lineare. Il motivo per il quale la linearità viene definita
sul tipo funzione e non sulla totalità dei tipi è che questo permette di avere retrocompatibilità su un type-system già
esistente. Per quanto riguarda la nozione di \textit{valutazione} (o \textit{consumazione}) di un valore, è bene
introdurre la politica che viene utilizzata da Haskell: la \textit{valutazione lazy}. Questo tipo di valutazione delle
espressioni permette di valutare le espressioni non nel momento in cui vengono legate alle variabili,
bensì quando c'è l'effettivo bisogno del loro risultato per effettuare altre computazioni. Questa politica permette di
ritardare il più possibile la valutazione delle espressioni.
Precisiamo cosa si intende con \textit{consumare un valore esattamente una volta}, diamo questa
definizione proposta anche nell'articolo di riferimento:
\begin{itemize}
    \item per consumare un valore atomico (ovvero di un tipo con costruttore di tipo senza argomenti, ad esempio il
    tipo \texttt{Char}) esattamente una volta, lo si valuti;
    \item per consumare un valore di tipo funzione esattamente una volta, si applichi a esso un argomento e consuma
    il risultato esattamente una volta;
    \item per consumare un valore di un tipo di dato algebrico esattamente una volta, si esegua il pattern matching su
    di esso e si consumi tutte le sue componenti lineari esattamente una volta.
\end{itemize}
Con questa definizione possiamo arrivare a una prima conclusione sul tipo dei data-constructors, ovvero quale tipo
dovremmo assegnare a un data-constructor? Prendiamo come esempio il costruttore delle coppie \texttt{(,)}:
\begin{itemize}
    \item $ (,) : \forall \alpha, \beta. \; \alpha \; \triangleright \; \beta \; \triangleright \; (\alpha, \beta) $
    \item $ (,) : \forall \alpha, \beta. \; \alpha \mapsto \beta \mapsto (\alpha, \beta) $
\end{itemize}
La prima scelta è quella corretta, infatti, se il risultato dell'espressione \texttt{(,) x y} è consumato esattamente
una volta, allora, per definizione, \texttt{x} e \texttt{y} vengono consumati esattamente una volta.

Questo sistema di tipi può portare, tuttavia, ad alcuni problemi. Si pensi, come viene mostrato in un esempio in [20],
alla seguente implementazione della funzione \texttt{map} di Haskell:
\begin{lstlisting}
map _ [] = []
map f (x : t) = f x : map f t
\end{lstlisting}
La funzione \texttt{map} può avere i seguenti due tipi:
\begin{itemize}
    \item $ map : \forall \alpha, \beta. \; (\alpha \; \triangleright \; \beta) \mapsto [\alpha] \; \triangleright \; [\beta] $
    \item $ map : \forall \alpha, \beta. \; (\alpha \mapsto \beta) \mapsto [\alpha] \mapsto [\beta] $
\end{itemize}
I due tipi sono incompatibili tra loro. Introduciamo, quindi, un livello di polimorfismo sulla
\textit{molteplicità} delle freccie (tipi funzione):
\begin{itemize}
    \item $ map : \forall \alpha, \beta, p. \; (\alpha \mapsto_p \beta) \mapsto [\alpha] \mapsto_p [\beta] $
\end{itemize}
Con la notazione appena mostrata, stiamo asserendo che \texttt{map} accetta come argomenti una funzione con molteplicità
illimitata (ovvero non-lineare) e una lista con molteplicità $ p $, inoltre, la funzione in input accetta un argomento
di molteplicità $ p $. Definiamo formalmente la molteplicità:
\[ \pi, \mu := 1 \; | \; \omega \; | \; p \; | \; \pi + \mu \; | \; \pi * \mu \]
dove $ \omega $ è la molteplicità illimitata e $ p $ è una variabile di molteplicità. Inoltre, valgono le seguenti
relazioni affermazioni:
\begin{itemize}
    \item $ + $ e $ * $ sono associativi e commutativi;
    \item $ * $ ha precedenza maggiore di $ + $;
    \item $ \forall p. \; p * 1 = 1 * p = p $;
    \item $ \omega * \omega = \omega $;
    \item $ 1 + 1 = 1 + \omega = \omega + \omega = \omega $.
\end{itemize}
Consideriamo $ \mapsto \; \equiv \; \mapsto_{\omega} $ e $ \triangleright \; \equiv \; \mapsto_1 $. Si noti come valga
la seguente affermazione:
\[ \forall \pi, \mu. \; \pi + \mu = \omega \]
Tuttavia, se aggiungiamo l'elemento neutro di $ + $, sia esso $ 0 $, allora la precedente affermazione non vale più.
Dato questo sistema di tipi lineari, è ora necessario aggiornare l'algoritmo di type inference in modo che inferisca
anche le molteplicità dei vari tipi funzione. Per questo, in [20] viene definito un lambda-calcolo le cui regole
d'inferenza inferiscono la molteplicità corretta dei tipi funzione. Non specificheremo di seguito le regole d'inferenza,
tuttavia, è bene fare alcune precisazioni. Innanzitutto, nelle regole di inferenza, si può pensare ai tipi nell'ambiente
di tipizzazione come l'input di un giudizio e alle molteplicità come l'output.
Esistono due regole diverse per quanto riguarda le variabili e i data-constructors e questo è dato dalla diversa
definizione di linearità per le funzioni e per i data-constructors. Per quanto riguarda, invece, il costrutto di
lambda-astrazione, visto che deve essere creato un tipo funzione e a priori non si conosce la molteplicità di questo
tipo, a esso viene associata una variabile di molteplicità (che non esiste già come variabile libera). Per la regola
di applicazione, è necessario guardare la
molteplicità della freccia (nel tipo dell'espressione che applica) e a seconda della sua molteplcità, le variabili libere
nell'argomento dell'applicazione dovranno essere utilizzate con la stessa molteplicità.

\paragraph{Bozza di implementazione in langgg}
Ora che è stato presentato un sistema di tipi lineari, si può delineare una bozza di implementazione in langgg.
Il sistema di tipi lineari da aggiungere a quello già esistente di langgg si ispira a quello appena presentato,
riutilizzando le varie nozioni incontrate. Ma prima di aggiornare il sistema di tipi di langgg, è doveroso definire
la politica di valutazione delle espressioni in langgg (al momento indefinita e dipendente dall'implementazione).
Una scelta che può essere coerente con il sistema di tipi lineari da aggiungere è quella di una politica \textit{lazy}
come quella di Haskell. Questo comporta che la traduzione dei token tipati in codice Core sia tale che le espressioni
di langgg legate alle variabili non vengano valutate al momento della definizione delle variabili, bensì, la loro
valutazione venga ritardata fin quando ci sarà una computazione all'interno del programma che necessiterà il loro
valore. Fatta questa importante premessa, ora è possibile discutere l'estensione del sistema di tipi di langgg.
Un primo aggiornamento che può essere effettuato è alla nozione di mono-tipo:
\[ MT \; := \; \alpha \; | \; T \; | \; MT \; MT \; | \; MT \mapsto_{\pi} MT \]
Dove $ \pi $ è la nozione di molteplicità fornita in precedenza (anch'essa da aggiungere al sistema di tipi).
Dopodiché, sarà necessario estendere l'algoritmo di type inference, in particolare:
\begin{itemize}
    \item è necessario aggiungere un "attore" all'ambiente di tipizzazione, sia esso $ \Pi $, il quale associa ai
    token una molteplicità. Anche in questo ambiente sarà necessario tener conto delle variabili libere, in quanto
    la nozione di molteplicità stessa può possedere variabili di molteplicità;
    \item estendere le regole di inferenza in modo che, oltre alla normale type inference, eseguano anche l'inferenza
    di molteplicità. Si presume che, come per i tipi, in langgg le molteplicità possano o non possano essere esplicitate;
    \item per quanto riguarda le regole di inferenza, al momento della generalizzazione, sarà necessario generalizzare
    anche le eventuali variabili di molteplicità che non compaiono libere in $ \Pi $.
\end{itemize}

\section*{Bibliografia}
\begin{enumerate}[label={[\arabic*]}]
    %[1]
    \item Amr Sabry - What is a purely functional language? - in: Journal of Functional Programming, Volume 8, Issue 1,
    January 1998, pp. 1 - 22
    %[2]
    \item Martin Sulzmann, Martin Odersky, Martin Wehr - Type Inference with Constrained Types - in: Theory and Practice
    of Object Systems · January 1999
    %[3]
    \item Stephen Diehl - Dive into GHC: Targeting Core - url: \url{https://www.stephendiehl.com/posts/ghc_03.html}
    %[4]
    \item libreria Parsec - url: \url{https://hackage.haskell.org/package/parsec}
    %[5]
    \item Data types for Haskell entities - url:
    \url{https://gitlab.haskell.org/ghc/ghc/-/wikis/commentary/compiler/entity-types}
    %[6]
    \item Edward Y. Zang - Backpack without symbol tables - in: May 11, 2016 - url:
    \url{http://web.mit.edu/~ezyang/Public/backpack-symbol-tables.pdf}
    %[7]
    \item Haskell Flexible Instances extension - url:
    \url{https://ghc.gitlab.haskell.org/ghc/doc/users_guide/exts/instances.html#extension-FlexibleInstances}
    %[8]
    \item Haskell instance resolution termination conditions - url:
    \url{https://ghc.gitlab.haskell.org/ghc/doc/users_guide/exts/instances.html#instance-termination-rules}
    %[9]
    \item Martin Sulzmann, Gregory J. Duck, Simon Peyton-Jones, Peter J. Stuckey - Understanding Functional Dependencies
    via Constraint Handling Rules - in: Journal of Functional Programming, Volume 17, Issue 1, January 2007, pp. 83 - 129
    %[10]
    \item Definizione di ClsInst in GHC - url: \url{https://hackage.haskell.org/package/ghc-8.10.7/docs/src/InstEnv.html#ClsInst}
    %[11]
    \item Monomorphization - url: \url{https://doc.rust-lang.org/book/ch10-01-syntax.html#performance-of-code-using-generics}
    %[12]
    \item Type inference for Haskell, part 15 - url: \url{https://jeremymikkola.com/posts/2019_01_15_type_inference_for_haskell_part_15.html}
    %[13]
    \item Overlapping instances in Haskell - url: \url{https://ghc.gitlab.haskell.org/ghc/doc/users_guide/exts/instances.html#overlapping-instances}
    %[14]
    \item The GHC API, version 8.10.7 - url: \url{https://hackage.haskell.org/package/ghc-8.10.7}
    %[15]
    \item CoreSyn, version 8.10.7 - url: \url{https://hackage.haskell.org/package/ghc-8.10.7/docs/CoreSyn.html}
    %[16]
    \item Type, version 8.10.7 - url: \url{https://hackage.haskell.org/package/ghc-8.10.7/docs/Type.html}
    %[17]
    \item TyCon, version 8.10.7 - url: \url{https://hackage.haskell.org/package/ghc-8.10.7/docs/TyCon.html}
    %[18]
    \item DataCon, version 8.10.7 - url: \url{https://hackage.haskell.org/package/ghc-8.10.7/docs/DataCon.html}
    %[19]
    \item GHC Commentary: The Compiler - url: \url{https://gitlab.haskell.org/ghc/ghc/-/wikis/commentary/compiler/}
    %[20]
    \item Jean-Philippe Bernardy, Mathieu Boespflug, Ryan R. Newton, Simon Peyton Jones, Arnaud Spiwack - Linear Haskell -
    in: arXiv:1710.09756
\end{enumerate}

\end{document}
